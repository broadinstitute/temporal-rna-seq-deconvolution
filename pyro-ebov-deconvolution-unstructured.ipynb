{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vietnamese-memory",
   "metadata": {},
   "source": [
    "## EBOV deconvolution in Pyro Unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-revolution",
   "metadata": {},
   "source": [
    "```\n",
    "Indices:\n",
    "\n",
    "- c cell type\n",
    "- g genes\n",
    "- m samples\n",
    "- n sample in batch\n",
    "- k deformation polynomial degree\n",
    "- i individual\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chief-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "device = torch.device(\"cpu:0\")\n",
    "dtype = torch.float32\n",
    "dtype_np = np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-accent",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "funky-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/nbarkas/disk3/ebov_bulk_rna_seq/proc_20210524/18-CompositionDeconvolution/03-previous_pyro_decov/deconvolution/data/load_data_python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "provincial-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from boltons.cacheutils import cachedproperty\n",
    "\n",
    "\n",
    "class EBOVDataset:\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: str,\n",
    "            dtype_np: np.dtype):\n",
    "        \n",
    "        self.dtype_np = dtype_np\n",
    "        \n",
    "        # load all data\n",
    "        self.bulk_raw_gex_gm = np.load(os.path.join(data_path, 'bulk_matrix.npy')).astype(dtype_np)\n",
    "        \n",
    "        with open(os.path.join(data_path, 'bulk_pdata.pkl'), 'rb') as f:\n",
    "            loader = pickle.Unpickler(f)\n",
    "            self.bulk_sample_metadata_pd = loader.load()\n",
    "\n",
    "        self.sc_raw_gex_gj = np.load(os.path.join(data_path, 'sc_matrix.npy'))\n",
    "\n",
    "        with open(os.path.join(data_path, 'sc_pdata.pkl'), 'rb') as f:\n",
    "            loader = pickle.Unpickler(f)\n",
    "            self.sc_sample_metadata_pd = loader.load()\n",
    "            \n",
    "        # shorthands\n",
    "        self.num_samples = self.bulk_raw_gex_gm.shape[-1]\n",
    "        self.num_genes = self.bulk_raw_gex_gm.shape[-2]\n",
    "        self.num_individuals = len(set(self.bulk_sample_metadata_pd['id.individual'].values))\n",
    "        \n",
    "        # index individuals\n",
    "        individual_str_list = sorted(list(set(self.bulk_sample_metadata_pd['id.individual'].values)))\n",
    "        individual_str_to_index_map = {\n",
    "            individual_str: index for index, individual_str in enumerate(individual_str_list)}\n",
    "        \n",
    "        # individual index of each sample\n",
    "        self.m_to_i = np.asarray(list(\n",
    "            map(\n",
    "                individual_str_to_index_map.get,\n",
    "                self.bulk_sample_metadata_pd['id.individual'].values)))\n",
    "        \n",
    "        # pre-process dpi times\n",
    "        self.dpi_time_m = np.clip(\n",
    "            self.bulk_sample_metadata_pd['dpi_time'].values.astype(dtype_np),\n",
    "            a_min=0., a_max=None)\n",
    "        self.dpi_time_m = self.dpi_time_m / self.dpi_time_m.max()\n",
    "\n",
    "    @cachedproperty\n",
    "    def cell_type_str_list(self) -> List[str]:\n",
    "        return sorted(list(set(self.sc_sample_metadata_pd['Subclustering_reduced'])))\n",
    "\n",
    "    @cachedproperty\n",
    "    def cell_type_str_to_index_map(self) -> Dict[str, int]:\n",
    "        return {cell_type_str: index for index, cell_type_str in enumerate(self.cell_type_str_list)}        \n",
    "\n",
    "    @cachedproperty\n",
    "    def num_cell_types(self) -> int:\n",
    "        return len(self.cell_type_str_list)\n",
    "    \n",
    "    @cachedproperty\n",
    "    def w_hat_gc(self) -> np.ndarray:\n",
    "        w_hat_gc = np.zeros((self.num_genes, self.num_cell_types))\n",
    "        for cell_type_str in self.cell_type_str_list:\n",
    "            i_cell_type = self.cell_type_str_to_index_map[cell_type_str]\n",
    "            mask_j = self.sc_sample_metadata_pd['Subclustering_reduced'].values == cell_type_str\n",
    "            w_hat_gc[:, i_cell_type] = np.sum(self.sc_raw_gex_gj[:, mask_j], axis=-1)\n",
    "            w_hat_gc[:, i_cell_type] = w_hat_gc[:, i_cell_type] / np.sum(w_hat_gc[:, i_cell_type])\n",
    "        return w_hat_gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rational-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebov_dataset = EBOVDataset(data_path, dtype_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "million-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def generate_minibatch(\n",
    "        dataset: EBOVDataset,\n",
    "        minibatch_size: int,\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype,\n",
    "        return_full_batch: bool = False) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    \n",
    "    TODO: balanced sampling from time\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # choose random samples\n",
    "    if return_full_batch:\n",
    "        sample_indices_n = np.arange(dataset.num_samples)\n",
    "    else:\n",
    "        sample_indices_n = np.random.randint(0, dataset.num_samples, size=minibatch_size)\n",
    "    \n",
    "    # subset data\n",
    "    x_ng = dataset.bulk_raw_gex_gm[:, sample_indices_n].T\n",
    "    t_n = dataset.dpi_time_m[sample_indices_n]\n",
    "    ind_index_n = dataset.m_to_i[sample_indices_n]\n",
    "    \n",
    "    return {\n",
    "        'x_ng': torch.tensor(x_ng, device=device, dtype=dtype),\n",
    "        't_n': torch.tensor(t_n, device=device, dtype=dtype),\n",
    "        'ind_index_n': torch.tensor(ind_index_n, device=device, dtype=torch.long),\n",
    "        'sample_index_n': torch.tensor(sample_indices_n, device=device, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "relevant-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions.torch_distribution import TorchDistribution, TorchDistributionMixin\n",
    "from torch.distributions.utils import probs_to_logits, logits_to_probs, broadcast_all, lazy_property\n",
    "from torch.distributions import constraints\n",
    "from numbers import Number\n",
    "\n",
    "def NegativeBinomialAltParam(mu, phi):\n",
    "    r\"\"\"\n",
    "    Creates a negative binomial distribution.\n",
    "    \n",
    "    Args:\n",
    "        mu (Number, Tensor): mean (must be strictly positive)\n",
    "        phi (Number, Tensor): overdispersion (must be strictly positive)\n",
    "    \"\"\"\n",
    "    return dist.GammaPoisson(\n",
    "            concentration= 1 /phi ,\n",
    "            rate=1 / (mu * phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69a3f1c-b917-498a-88d3-06b17d8d93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyro.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192ad1ee-dfb4-44c8-877d-010a212d520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyro.distributions import Dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9bc32d9-204c-4552-82c2-6935c2780504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as pyro_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46fff32c-1a6a-4551-8988-a210598b15e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,1]).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d704ff9b-8edd-4dc9-8618-ff8a9bc14798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4877, 0.5123])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.sample(\"asdf\",pyro_dist.Dirichlet(concentration=torch.tensor([1.,1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "\n",
    "\n",
    "class PseudoTimeRegDeconvolution:\n",
    "    \"\"\"\n",
    "    \n",
    "    .. note: we use \"n\" for batch dimension, and \"m\" for sample index.\n",
    "    .. note: for polynomial degree use k\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def  __init__(\n",
    "            self,\n",
    "            dataset: EBOVDataset,\n",
    "            polynomial_degree: int,\n",
    "            device: device,\n",
    "            dtype: dtype):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.polynomial_degree = polynomial_degree\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.log_beta_prior_scale = 1.0\n",
    "        self.log_r_prior_scale = 1.0\n",
    "        self.tau_prior_scale = 1.0\n",
    "        self.log_phi_prior_loc = -5.0\n",
    "        self.log_phi_prior_scale = 1.0\n",
    "        \n",
    "        self.unnorm_cell_pop_base_prior_loc_c = np.zeros((self.dataset.num_cell_types,))\n",
    "        self.unnorm_cell_pop_base_prior_scale_c = np.ones((self.dataset.num_cell_types,))\n",
    "        \n",
    "        # dist of coefficients of population deformation polynomial\n",
    "        self.unnorm_cell_pop_deform_prior_loc_ck = np.zeros((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "        self.unnorm_cell_pop_deform_prior_scale_ck = np.ones((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "\n",
    "        self.init_posterior_global_scale_factor = 0.05\n",
    "        \n",
    "        self.log_beta_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.log_r_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.tau_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.log_phi_posterior_loc = -5.0\n",
    "        self.log_phi_posterior_scale = 0.1 * self.init_posterior_global_scale_factor\n",
    "        \n",
    "        \n",
    "        self.unnorm_cell_pop_base_posterior_loc_c = np.zeros((self.dataset.num_cell_types,))\n",
    "        self.unnorm_cell_pop_base_posterior_scale_c = self.init_posterior_global_scale_factor * np.ones((self.dataset.num_cell_types,))\n",
    "        \n",
    "        self.unnorm_cell_pop_deform_posterior_loc_ck = np.zeros((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "        self.unnorm_cell_pop_deform_posterior_scale_ck = self.init_posterior_global_scale_factor *  np.ones((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "        \n",
    "        # cache useful tensors\n",
    "        self.w_hat_gc = torch.tensor(self.dataset.w_hat_gc, device=device, dtype=dtype)\n",
    "        \n",
    "    def model(\n",
    "            self,\n",
    "            x_ng: torch.Tensor,\n",
    "            ind_index_n: torch.Tensor,\n",
    "            sample_index_n: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x_ng: gene expression\n",
    "        :param ind_index_n: index of the individual\n",
    "        :param sample_index_n: index of the sample in the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        minibatch_size = x_ng.shape[0]\n",
    "        \n",
    "        # sample log_phi_g\n",
    "        log_phi_g = pyro.sample(\n",
    "            'log_phi_g',\n",
    "            dist.Normal(\n",
    "                loc=self.log_phi_prior_loc * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype),\n",
    "                scale=self.log_phi_prior_scale * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype)).to_event(1))\n",
    "        \n",
    "        # sample log_beta_g\n",
    "        log_beta_g = pyro.sample(\n",
    "            'log_beta_g',\n",
    "            dist.Normal(\n",
    "                loc=torch.zeros(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype),\n",
    "                scale=self.log_beta_prior_scale * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype)).to_event(1))\n",
    "        \n",
    "        # sample unnorm_cell_pop_base_c\n",
    "        unnorm_cell_pop_base_c = pyro.sample(\n",
    "            'unnorm_cell_pop_base_c',\n",
    "            dist.Normal(\n",
    "                loc=torch.tensor(\n",
    "                    self.unnorm_cell_pop_base_prior_loc_c,\n",
    "                    device=self.device, dtype=self.dtype),\n",
    "                scale=torch.tensor(\n",
    "                    self.unnorm_cell_pop_base_prior_scale_c,\n",
    "                    device=self.device, dtype=self.dtype)).to_event(1))\n",
    "        \n",
    "        # calculate useful derived variables\n",
    "        beta_g = log_beta_g.exp()\n",
    "        phi_g = log_phi_g.exp()\n",
    "          \n",
    "        unnorm_w_gc = self.w_hat_gc * beta_g[:, None]\n",
    "        w_gc = unnorm_w_gc / unnorm_w_gc.sum(0)\n",
    "        \n",
    "        with pyro.poutine.scale(scale=self.dataset.num_samples / minibatch_size):\n",
    "            \n",
    "            with pyro.plate(\"batch\", minibatch_size):\n",
    "                \n",
    "                # sample directly from a Dirichlet\n",
    "                cell_pop_unnorm_nc = pyro.sample(\"\", dist.Dirichlet())\n",
    "                cell_pop_nc = torch.nn.functional.softmax(cell_pop_unnorm_nc, dim=-1)\n",
    "                \n",
    "                # calculate mean gene expression\n",
    "                mu_ng = x_ng.sum(-1)[:, None] * torch.matmul(cell_pop_nc, w_gc.transpose(-1, -2))\n",
    "\n",
    "                # observe gene expression\n",
    "                pyro.sample(\n",
    "                    \"x_ng\",\n",
    "                    NegativeBinomialAltParam(\n",
    "                        mu=mu_ng,\n",
    "                        phi=phi_g[None, :]).to_event(1),\n",
    "                    obs=x_ng)\n",
    "\n",
    "    def delta_guide(\n",
    "            self,\n",
    "            x_ng: torch.Tensor,\n",
    "            t_n: torch.Tensor,\n",
    "            ind_index_n: torch.Tensor,\n",
    "            sample_index_n: torch.Tensor):\n",
    "        \n",
    "        minibatch_size = x_ng.shape[0]\n",
    "        \n",
    "        # Change guide to auto-delta\n",
    "        # pyro.infer.autoguide.AutoDelta \n",
    "        \n",
    "        # variational parameters for log_phi_g\n",
    "        log_phi_posterior_loc_g = pyro.param(\n",
    "            \"log_phi_posterior_loc_g\",\n",
    "            self.log_phi_posterior_loc * torch.ones(\n",
    "                (self.dataset.num_genes,), device=self.device, dtype=self.dtype))\n",
    "\n",
    "        # variational parameters for log_beta_g\n",
    "        log_beta_posterior_loc_g = pyro.param(\n",
    "            \"log_beta_posterior_loc_g\",\n",
    "            torch.zeros(\n",
    "                (self.dataset.num_genes,), device=self.device, dtype=self.dtype))        \n",
    "        \n",
    "        # variational parameters for log_r_i\n",
    "        log_r_posterior_loc_i = pyro.param(\n",
    "            \"log_r_posterior_loc_i\",\n",
    "            torch.zeros(\n",
    "                (self.dataset.num_individuals,), device=self.device, dtype=self.dtype))\n",
    "        \n",
    "        # variational parameters for unnorm_cell_pop_base_c (\"B_c\")\n",
    "        unnorm_cell_pop_base_posterior_loc_c = pyro.param(\n",
    "            \"unnorm_cell_pop_base_posterior_loc_c\",\n",
    "            torch.tensor(\n",
    "                self.unnorm_cell_pop_base_posterior_loc_c,\n",
    "                device=self.device, dtype=self.dtype))\n",
    "        \n",
    "        # variational parameters for unnorm_cell_pop_deform_c (\"R_c\")\n",
    "        unnorm_cell_pop_deform_posterior_loc_ck = pyro.param(\n",
    "            \"unnorm_cell_pop_deform_posterior_loc_ck\",\n",
    "            torch.tensor(\n",
    "                self.unnorm_cell_pop_deform_posterior_loc_ck,\n",
    "                device=self.device, dtype=self.dtype))\n",
    "        \n",
    "        # variational parameters for tau_deviation_m\n",
    "        tau_deviation_posterior_loc_m = pyro.param(\n",
    "            \"tau_deviation_posterior_loc_m\",\n",
    "            torch.zeros(\n",
    "                (self.dataset.num_samples,),\n",
    "                device=self.device, dtype=self.dtype))\n",
    "        \n",
    "        # posterior sample statements\n",
    "        log_phi_g = pyro.sample(\n",
    "            \"log_phi_g\",\n",
    "            dist.Delta(\n",
    "                v=log_phi_posterior_loc_g).to_event(1))\n",
    "\n",
    "        log_beta_g = pyro.sample(\n",
    "            \"log_beta_g\",\n",
    "            dist.Delta(\n",
    "                v=log_beta_posterior_loc_g).to_event(1))\n",
    "\n",
    "        log_r_i = pyro.sample(\n",
    "            \"log_r_i\",\n",
    "            dist.Delta(\n",
    "                v=log_r_posterior_loc_i).to_event(1))\n",
    "\n",
    "        unnorm_cell_pop_base_c = pyro.sample(\n",
    "            'unnorm_cell_pop_base_c',\n",
    "            dist.Delta(\n",
    "                v=unnorm_cell_pop_base_posterior_loc_c).to_event(1))\n",
    "        \n",
    "        # Ask MB about to_event() here\n",
    "        unnorm_cell_pop_deform_ck = pyro.sample(\n",
    "            'unnorm_cell_pop_deform_ck',\n",
    "            dist.Delta(v=unnorm_cell_pop_deform_posterior_loc_ck).to_event(2))\n",
    "        \n",
    "        \n",
    "        with pyro.poutine.scale(scale=self.dataset.num_samples / minibatch_size):\n",
    "\n",
    "            with pyro.plate(\"batch\"):\n",
    "            \n",
    "                tau_deviation_n = pyro.sample(\n",
    "                    \"tau_deviation_n\",\n",
    "                    dist.Delta(\n",
    "                        v=tau_deviation_posterior_loc_m[sample_index_n]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv = PseudoTimeRegDeconvolution(\n",
    "    dataset=ebov_dataset,\n",
    "    polynomial_degree = 6,\n",
    "    device=device,\n",
    "    dtype=dtype)\n",
    "\n",
    "pyro.clear_param_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 2_000\n",
    "minibatch_size = 50\n",
    "log_frequency = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optim = pyro.optim.Adam(\n",
    "    {'lr': 5e-3})\n",
    "\n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVI objective\n",
    "svi = SVI(\n",
    "    model=pseudo_time_reg_deconv.model,\n",
    "    guide=pseudo_time_reg_deconv.delta_guide,\n",
    "    optim=optim,\n",
    "    loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_iter in range(n_iters):\n",
    "    \n",
    "    minibatch_dict = generate_minibatch(\n",
    "        ebov_dataset, minibatch_size, device, dtype,\n",
    "        return_full_batch=True)\n",
    "    \n",
    "    loss = svi.step(**minibatch_dict)\n",
    "    \n",
    "    loss_hist.append(loss)\n",
    "    \n",
    "    if i_iter % log_frequency == 0:\n",
    "        print(f\"{i_iter}   loss: {loss_hist[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_composition_trajectories(time_step=0.01):\n",
    "    times_z = torch.arange(0,1,time_step)\n",
    "    # Take time to appropriate exponent\n",
    "    times_zk = torch.pow(times_z[:,None], torch.arange(1,pseudo_time_reg_deconv.polynomial_degree + 1,))\n",
    "    # get the trained params\n",
    "    base_composition_post_c = pyro.param(\"unnorm_cell_pop_base_posterior_loc_c\").detach().cpu()\n",
    "    delta_composition_post_ck = pyro.param(\"unnorm_cell_pop_deform_posterior_loc_ck\").detach().cpu()\n",
    "    # Calculate the deltas for each time point\n",
    "    delta_cz = torch.matmul(delta_composition_post_ck, times_zk.transpose(-1,-2))\n",
    "    # normalize\n",
    "    norm_comp_t = torch.nn.functional.softmax(base_composition_post_c[:,None] + delta_cz, dim=0).numpy().T\n",
    "    return {\"times_z\": times_z.numpy(), \"norm_comp_t\": norm_comp_t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "composition_results = get_composition_trajectories(time_step=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_composition_trajectories(composition_trajectories):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.plot(composition_trajectories[\"times_z\"], composition_trajectories[\"norm_comp_t\"])\n",
    "    ax.set_title(\"Predicted cell proportions\")\n",
    "    ax.set_xlabel(\"Pseudotime\")\n",
    "    ax.legend(ebov_dataset.cell_type_str_list,loc='best',fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_composition_trajectories(composition_results)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
