{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial-grace",
   "metadata": {},
   "source": [
    "## Ebola infected Macaque Sample Composition Trajectory Identification\n",
    "\n",
    "In this notebook we deconvolve the ebov macaque blood samples using a matched seq-well reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ternadecov.time_deconv import *\n",
    "from ternadecov.simulator import *\n",
    "from ternadecov.stats_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-junction",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float32\n",
    "dtype_np = np.float32\n",
    "n_iters = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-specification",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_anndata_path = \"/home/jupyter/mb-ml-data-disk/temporal-rna-seq-deconvolution/ebov_bulk.h5ad\"\n",
    "sc_anndata_path = \"/home/jupyter/mb-ml-data-disk/temporal-rna-seq-deconvolution/ebov_sc.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bulk_anndata_path, 'rb') as fh:\n",
    "    bulk_anndata = anndata.read_h5ad(fh)\n",
    "with open(sc_anndata_path, 'rb') as fh:\n",
    "    sc_anndata = anndata.read_h5ad(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select samples only after or on tp 0 \n",
    "bulk_anndata = bulk_anndata[bulk_anndata.obs['dpi_time'] >= 0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebov_dataset = DeconvolutionDataset(\n",
    "    sc_anndata = sc_anndata,\n",
    "    sc_celltype_col = \"Subclustering_reduced\",\n",
    "    bulk_anndata = bulk_anndata,\n",
    "    bulk_time_col = \"dpi_time\",\n",
    "    dtype_np = dtype_np,\n",
    "    dtype = dtype,\n",
    "    device=device,\n",
    "    feature_selection_method = 'single_cell_od' #'overdispersed_bulk'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-creativity",
   "metadata": {},
   "source": [
    "## `trajectories.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "from torch.distributions import constraints\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from typing import List, Dict\n",
    "import pyro.distributions as dist\n",
    "import anndata\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import math\n",
    "import tqdm\n",
    "import copy\n",
    "from matplotlib.pyplot import cm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scanpy as sc\n",
    "\n",
    "from ternadecov.stats_helpers import *\n",
    "from ternadecov.simulator import *\n",
    "from ternadecov.stats_helpers import *\n",
    "from ternadecov.hypercluster import *\n",
    "from ternadecov.dataset import *\n",
    "\n",
    "\n",
    "class BasicTrajectoryModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        basis_functions: str,\n",
    "        polynomial_degree: int,\n",
    "        num_cell_types: int,\n",
    "        num_samples: int,\n",
    "        init_posterior_global_scale_factor: float,\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.polynomial_degree = polynomial_degree\n",
    "        self.basis_functions = basis_functions\n",
    "        self.num_cell_types = num_cell_types\n",
    "        self.num_samples = num_samples\n",
    "        self.init_posterior_global_scale_factor = init_posterior_global_scale_factor\n",
    "\n",
    "        #####################################################\n",
    "        ## Prior\n",
    "        #####################################################\n",
    "        self.unnorm_cell_pop_base_prior_loc_c = np.zeros((self.num_cell_types,))\n",
    "        self.unnorm_cell_pop_base_prior_scale_c = np.ones((self.num_cell_types,))\n",
    "\n",
    "        # dist of coefficients of population deformation polynomial\n",
    "        self.unnorm_cell_pop_deform_prior_loc_ck = np.zeros(\n",
    "            (self.num_cell_types, self.polynomial_degree)\n",
    "        )\n",
    "        self.unnorm_cell_pop_deform_prior_scale_ck = np.ones(\n",
    "            (self.num_cell_types, self.polynomial_degree)\n",
    "        )\n",
    "\n",
    "        # Per sample celltype proportions\n",
    "        self.cell_pop_prior_loc_cm = (\n",
    "            np.ones((self.num_cell_types, self.num_samples)) / self.num_cell_types\n",
    "        )\n",
    "\n",
    "        # Dirichlet_alpha prior\n",
    "        self.dirichlet_alpha_prior = np.ones((1,)) * 1e5\n",
    "\n",
    "        #####################################################\n",
    "        ## Posterior\n",
    "        #####################################################\n",
    "\n",
    "        self.unnorm_cell_pop_base_posterior_loc_c = np.zeros((self.num_cell_types,))\n",
    "        self.unnorm_cell_pop_base_posterior_scale_c = (\n",
    "            self.init_posterior_global_scale_factor * np.ones((self.num_cell_types,))\n",
    "        )\n",
    "\n",
    "        self.unnorm_cell_pop_deform_posterior_loc_ck = np.zeros(\n",
    "            (self.num_cell_types, self.polynomial_degree)\n",
    "        )\n",
    "        self.unnorm_cell_pop_deform_posterior_scale_ck = (\n",
    "            self.init_posterior_global_scale_factor\n",
    "            * np.ones((self.num_cell_types, self.polynomial_degree))\n",
    "        )\n",
    "\n",
    "        self.cell_pop_posterior_loc_mc = (\n",
    "            self.init_posterior_global_scale_factor\n",
    "            * np.ones((self.num_samples, self.num_cell_types))\n",
    "            / self.num_cell_types\n",
    "        )\n",
    "\n",
    "    def model(self, t_m: torch.Tensor):\n",
    "\n",
    "        # sample unnorm_cell_pop_base_c\n",
    "        unnorm_cell_pop_base_c = pyro.sample(\n",
    "            \"unnorm_cell_pop_base_c\",\n",
    "            dist.Normal(\n",
    "                loc=torch.tensor(\n",
    "                    self.unnorm_cell_pop_base_prior_loc_c,\n",
    "                    device=self.device,\n",
    "                    dtype=self.dtype,\n",
    "                ),\n",
    "                scale=torch.tensor(\n",
    "                    self.unnorm_cell_pop_base_prior_scale_c,\n",
    "                    device=self.device,\n",
    "                    dtype=self.dtype,\n",
    "                ),\n",
    "            ).to_event(1),\n",
    "        )\n",
    "        assert unnorm_cell_pop_base_c.shape == (self.num_cell_types,)\n",
    "\n",
    "        # Deformation scale is a learnable parameter now\n",
    "        unnorm_cell_pop_deform_prior_scale_ck = pyro.param(\n",
    "            \"unnorm_cell_pop_deform_prior_scale_ck\",\n",
    "            torch.tensor(\n",
    "                self.unnorm_cell_pop_deform_prior_scale_ck,\n",
    "                device=self.device,\n",
    "                dtype=self.dtype,\n",
    "            ),\n",
    "            constraint=constraints.positive,\n",
    "        )\n",
    "        assert unnorm_cell_pop_deform_prior_scale_ck.shape == (\n",
    "            self.num_cell_types,\n",
    "            self.polynomial_degree,\n",
    "        )\n",
    "\n",
    "        unnorm_cell_pop_deform_ck = pyro.sample(\n",
    "            \"unnorm_cell_pop_deform_ck\",\n",
    "            dist.Normal(\n",
    "                loc=torch.tensor(\n",
    "                    self.unnorm_cell_pop_deform_prior_loc_ck,\n",
    "                    device=self.device,\n",
    "                    dtype=self.dtype,\n",
    "                ),\n",
    "                scale=unnorm_cell_pop_deform_prior_scale_ck,\n",
    "            ).to_event(2),\n",
    "        )\n",
    "        assert unnorm_cell_pop_deform_ck.shape == (\n",
    "            self.num_cell_types,\n",
    "            self.polynomial_degree,\n",
    "        )\n",
    "\n",
    "        dirichlet_alpha = pyro.param(\n",
    "            \"dirichlet_alpha\",\n",
    "            torch.tensor(\n",
    "                self.dirichlet_alpha_prior, device=self.device, dtype=self.dtype,\n",
    "            ),\n",
    "            constraint=constraints.positive,\n",
    "        )\n",
    "        assert dirichlet_alpha.shape == (1,)\n",
    "\n",
    "        if self.basis_functions == \"polynomial\":\n",
    "            tau_km = torch.pow(\n",
    "                t_m[None, :],\n",
    "                torch.arange(1, self.polynomial_degree + 1, device=self.device)[\n",
    "                    :, None\n",
    "                ],\n",
    "            )\n",
    "            deformation_mc = torch.matmul(unnorm_cell_pop_deform_ck, tau_km).transpose(\n",
    "                -1, -2\n",
    "            )\n",
    "        elif self.basis_functions == \"legendre\":\n",
    "            # l -- power of the term of the legrenre polynomial\n",
    "            t_m_prime = 2 * t_m - 1  # discrete times in (-1,1)\n",
    "            t_lm = torch.pow(\n",
    "                t_m_prime[None, :],\n",
    "                torch.arange(0, self.polynomial_degree + 1, device=self.device)[\n",
    "                    :, None\n",
    "                ],\n",
    "            )\n",
    "            c_kl = legendre_coefficient_mat(self.polynomial_degree, dtype=dtype)[\n",
    "                1:,\n",
    "            ].to(\n",
    "                device\n",
    "            )  # drop constant term\n",
    "            intermediate_legenre_vals_km = torch.matmul(c_kl, t_lm)\n",
    "            deformation_mc = torch.matmul(\n",
    "                unnorm_cell_pop_deform_ck, intermediate_legenre_vals_km\n",
    "            ).transpose(-1, -2)\n",
    "\n",
    "        # The normalized underlying trajectories, serve as Dirichlet params\n",
    "        trajectory_mc = torch.nn.functional.softmax(\n",
    "            unnorm_cell_pop_base_c[None, :] + deformation_mc, dim=-1\n",
    "        )\n",
    "\n",
    "        per_sample_draw = True\n",
    "        if per_sample_draw:\n",
    "            # dirichlet_alpha = torch.tensor([1e4], device=self.device)\n",
    "            dirichlet_dist = dist.Dirichlet(\n",
    "                concentration=trajectory_mc * dirichlet_alpha\n",
    "            ).to_event(1)\n",
    "\n",
    "            cell_pop_mc = pyro.sample(\"cell_pop_mc\", dirichlet_dist)\n",
    "        else:\n",
    "            cell_pop_mc = trajectory_mc\n",
    "\n",
    "        assert cell_pop_mc.shape == (self.num_samples, self.num_cell_types,)\n",
    "\n",
    "        return cell_pop_mc\n",
    "\n",
    "    def guide(self):\n",
    "\n",
    "        # variational parameters for unnorm_cell_pop_base_c (\"B_c\")\n",
    "        unnorm_cell_pop_base_posterior_loc_c = pyro.param(\n",
    "            \"unnorm_cell_pop_base_posterior_loc_c\",\n",
    "            torch.tensor(\n",
    "                self.unnorm_cell_pop_base_posterior_loc_c,\n",
    "                device=self.device,\n",
    "                dtype=self.dtype,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # variational parameters for unnorm_cell_pop_deform_c (\"R_c\")\n",
    "        unnorm_cell_pop_deform_posterior_loc_ck = pyro.param(\n",
    "            \"unnorm_cell_pop_deform_posterior_loc_ck\",\n",
    "            torch.tensor(\n",
    "                self.unnorm_cell_pop_deform_posterior_loc_ck,\n",
    "                device=self.device,\n",
    "                dtype=self.dtype,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Cell composition\n",
    "        #         new_code = False\n",
    "        #         if new_code:\n",
    "        #              cell_pop_unconstrained_posterior_loc_mc = pyro.param(\n",
    "        #                 \"cell_pop_unconstrained_posterior_loc_mc\",\n",
    "        #                 torch.tensor(\n",
    "        #                     self.cell_pop_posterior_loc_mc,\n",
    "        #                     device=self.device,\n",
    "        #                     dtype=self.dtype,\n",
    "        #                 ),\n",
    "        #                 constraint=constraints.simplex,\n",
    "        #             )\n",
    "        #             epsilon = 1e-6\n",
    "        #             # This is on a simplex\n",
    "        #             cell_pop_posterior_loc_mc = epsilon / self.cell_pop_posterior_loc_mc.shape[-1] +\n",
    "        #                 (1-epsilon) * cell_pop_unconstrained_posterior_loc_mc\n",
    "        #         else:\n",
    "        cell_pop_posterior_loc_mc = pyro.param(\n",
    "            \"cell_pop_posterior_loc_mc\",\n",
    "            torch.tensor(\n",
    "                self.cell_pop_posterior_loc_mc, device=self.device, dtype=self.dtype,\n",
    "            ),\n",
    "            constraint=constraints.simplex,\n",
    "        )\n",
    "\n",
    "        unnorm_cell_pop_base_c = pyro.sample(\n",
    "            \"unnorm_cell_pop_base_c\",\n",
    "            dist.Delta(v=unnorm_cell_pop_base_posterior_loc_c).to_event(1),\n",
    "        )\n",
    "\n",
    "        unnorm_cell_pop_deform_ck = pyro.sample(\n",
    "            \"unnorm_cell_pop_deform_ck\",\n",
    "            dist.Delta(v=unnorm_cell_pop_deform_posterior_loc_ck).to_event(2),\n",
    "        )\n",
    "\n",
    "        cell_pop_mc = pyro.sample(\n",
    "            \"cell_pop_mc\", dist.Delta(v=cell_pop_posterior_loc_mc).to_event(2),\n",
    "        )\n",
    "\n",
    "    def calculate_composition_trajectories(\n",
    "        self, dataset, n_intervals=100, return_vals=False\n",
    "    ):\n",
    "        \"\"\"Calculate the composition trajectories\"\"\"\n",
    "        # calculate true times\n",
    "        if self.basis_functions == \"polynomial\":\n",
    "            time_step = 1 / n_intervals\n",
    "            times_z = torch.arange(0, 1, time_step)\n",
    "            # Take time to appropriate exponent\n",
    "            times_zk = torch.pow(\n",
    "                times_z[:, None], torch.arange(1, self.polynomial_degree + 1,),\n",
    "            )\n",
    "            # get the trained params\n",
    "            base_composition_post_c = (\n",
    "                pyro.param(\"unnorm_cell_pop_base_posterior_loc_c\").detach().cpu()\n",
    "            )\n",
    "            delta_composition_post_ck = (\n",
    "                pyro.param(\"unnorm_cell_pop_deform_posterior_loc_ck\").detach().cpu()\n",
    "            )\n",
    "            # Calculate the deltas for each time point\n",
    "            delta_cz = torch.matmul(\n",
    "                delta_composition_post_ck, times_zk.transpose(-1, -2)\n",
    "            )\n",
    "            # normalize\n",
    "            norm_comp_tc = (\n",
    "                torch.nn.functional.softmax(\n",
    "                    base_composition_post_c[:, None] + delta_cz, dim=0\n",
    "                )\n",
    "                .numpy()\n",
    "                .T\n",
    "            )\n",
    "            true_times_z = times_z * dataset.time_range + dataset.time_min\n",
    "        elif self.basis_functions == \"legendre\":\n",
    "            time_step = 2 / n_intervals\n",
    "            times_z = torch.arange(-1, 1, time_step)\n",
    "            # Take time to appropriate exponent\n",
    "            times_zk = torch.pow(\n",
    "                times_z[:, None], torch.arange(1, self.polynomial_degree + 1,),\n",
    "            )\n",
    "            # get the trained params\n",
    "            base_composition_post_c = (\n",
    "                pyro.param(\"unnorm_cell_pop_base_posterior_loc_c\").detach().cpu()\n",
    "            )\n",
    "            delta_composition_post_ck = (\n",
    "                pyro.param(\"unnorm_cell_pop_deform_posterior_loc_ck\").detach().cpu()\n",
    "            )\n",
    "            # Calculate the deltas for each time point\n",
    "            delta_cz = torch.matmul(\n",
    "                delta_composition_post_ck, times_zk.transpose(-1, -2)\n",
    "            )\n",
    "            # normalize\n",
    "            norm_comp_tc = (\n",
    "                torch.nn.functional.softmax(\n",
    "                    base_composition_post_c[:, None] + delta_cz, dim=0\n",
    "                )\n",
    "                .numpy()\n",
    "                .T\n",
    "            )\n",
    "            true_times_z = ((times_z + 1) / 2) * dataset.time_range + dataset.time_min\n",
    "\n",
    "        norm_comp_ct_torch = torch.Tensor(norm_comp_tc).T\n",
    "        summarized_composition_rt = None\n",
    "        toplevel_cell_map = None\n",
    "        if dataset.is_hyperclustered:\n",
    "            cluster_map = dataset.hypercluster_results[\"cluster_map\"]\n",
    "            toplevel_cell_map = {\n",
    "                ct: i for i, ct in enumerate({cluster_map[k] for k in cluster_map})\n",
    "            }\n",
    "            summarized_num_cells = len(toplevel_cell_map)\n",
    "            summarized_composition_rt = torch.zeros(\n",
    "                (summarized_num_cells, norm_comp_tc.shape[0])\n",
    "            )\n",
    "\n",
    "            for c_index in range(0, norm_comp_ct_torch.shape[0] - 1):\n",
    "                low_cluster_name = dataset.cell_type_str_list[c_index]\n",
    "                top_cluster_name = cluster_map[low_cluster_name]\n",
    "                summarized_composition_rt[toplevel_cell_map[top_cluster_name]].add_(\n",
    "                    norm_comp_ct_torch[c_index,]\n",
    "                )\n",
    "\n",
    "        self.calculated_trajectories = {\n",
    "            \"times_z\": times_z.numpy(),\n",
    "            \"true_times_z\": true_times_z,\n",
    "            \"norm_comp_tc\": norm_comp_tc,  # These are the trajectories on the native clusters\n",
    "            \"summarized_composition_rt\": summarized_composition_rt,  # These are the trajectories on the summarized results\n",
    "            \"toplevel_cell_map\": toplevel_cell_map,\n",
    "        }\n",
    "\n",
    "        if return_vals:\n",
    "            return self.calculated_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-jacket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-billy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-paragraph",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-agreement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.gp.models import VariationalSparseGP, VariationalGP\n",
    "from pyro.nn.module import PyroParam, pyro_method\n",
    "import pyro.contrib.gp.kernels as kernels\n",
    "from pyro.contrib.gp.parameterized import Parameterized\n",
    "\n",
    "from abc import abstractmethod\n",
    "\n",
    "\n",
    "class TrajectoryModule(Parameterized):\n",
    "    \"\"\"The base class of all trajectory modules.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TrajectoryModule, self).__init__()\n",
    "        \n",
    "    \n",
    "    @abstractmethod\n",
    "    def model(self, xi_mq: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"TBW.\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abstractmethod\n",
    "    def guide(self, xi_mq: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"TBW.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "\n",
    "class VSGPTrajectoryModule(TrajectoryModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            xi_mq: torch.Tensor,\n",
    "            num_cell_types: int,\n",
    "            init_posterior_global_scale_factor: float,\n",
    "            device: torch.device,\n",
    "            dtype: torch.dtype,\n",
    "        ):\n",
    "        \"\"\"TBW.\n",
    "        \n",
    "        :param xi_mq: covariate tensor with shape (num_sample, covariate_n_dim)\n",
    "        \n",
    "        .. note:: in the current model where the only covairate is time, covariate_n_dim == 1\n",
    "        \n",
    "        .. note:: The Gaussian process is specifying a function in a (num_cell_types)-dimensional\n",
    "          unconstrained Euclidean space. Applying softmax to this function give us the normalized\n",
    "          cell populations on the (num_cell_types)-dimensional simplex. We refer to the unnormalized\n",
    "          function as \"f\" for brevity in the code.\n",
    "          \n",
    "        \"\"\"\n",
    "        super(VSGPTrajectoryModule, self).__init__()\n",
    "        \n",
    "        self.xi_mq = xi_mq\n",
    "        self.num_cell_types = num_cell_types\n",
    "        self.init_posterior_global_scale_factor = init_posterior_global_scale_factor\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        assert xi_mq.ndim == 2\n",
    "        self.num_samples, self.covariate_n_dim = xi_mq.shape\n",
    "        \n",
    "        # todo: pull up to __init__ signature\n",
    "        self.num_inducing_points = 10\n",
    "        self.init_rbf_kernel_lengthscale = 0.5\n",
    "        self.init_rbf_kernel_variance = 0.5\n",
    "        self.init_whitenoise_kernel_variance = 0.1\n",
    "        self.gp_cholesky_jitter = 1e-4\n",
    "        \n",
    "        #####################################################\n",
    "        ## Prior\n",
    "        #####################################################\n",
    "\n",
    "        # kernel setup\n",
    "        kernel_rbf = kernels.RBF(\n",
    "            input_dim=self.covariate_n_dim,\n",
    "            variance=torch.tensor(self.init_rbf_kernel_variance, device=device, dtype=dtype),\n",
    "            lengthscale=torch.tensor(self.init_rbf_kernel_lengthscale, device=device, dtype=dtype))\n",
    "        kernel_whitenoise = kernels.WhiteNoise(\n",
    "            input_dim=self.covariate_n_dim,\n",
    "            variance=torch.tensor(self.init_whitenoise_kernel_variance, device=device, dtype=dtype))\n",
    "        kernel_full = kernels.Sum(kernel_rbf, kernel_whitenoise)\n",
    "\n",
    "        # mean output\n",
    "        self.gp_f_mean_c = PyroParam(\n",
    "            torch.zeros((self.num_cell_types,), device=device, dtype=dtype))\n",
    "\n",
    "        def f_mean_function(xi_nq: torch.Tensor):\n",
    "            \"\"\"Takes the covariate tensor with shape (batch_size, covariate_n_dim) and returns the function\n",
    "            mean with shape (num_cell_types, batch_size).\n",
    "            \n",
    "            .. note: the shape of the output of GP is permuted.\n",
    "            \"\"\"\n",
    "            assert xi_nq.ndim == 2\n",
    "            assert xi_nq.shape[-1] == self.covariate_n_dim\n",
    "            batch_size = xi_nq.shape[0]\n",
    "            return self.gp_f_mean_c[..., None].expand([self.num_cell_types, batch_size])\n",
    "\n",
    "        # initial position for the inducing points\n",
    "        x_mean, x_std = torch.mean(self.xi_mq).item(), torch.std(self.xi_mq).item()\n",
    "        self.Xu_init = x_mean + x_std * torch.randn(\n",
    "            self.num_inducing_points, self.covariate_n_dim,\n",
    "            device=device, dtype=dtype)\n",
    "\n",
    "        # instantiate VSGP model\n",
    "        self.gp = VariationalGP(\n",
    "            X=xi_mq,\n",
    "            y=None,\n",
    "            kernel=kernel_full,\n",
    "#             Xu=self.Xu_init,\n",
    "#             num_data=self.num_samples,\n",
    "            likelihood=None,\n",
    "            mean_function=f_mean_function,\n",
    "            latent_shape=torch.Size([self.num_cell_types]),\n",
    "            whiten=True,\n",
    "            jitter=self.gp_cholesky_jitter)\n",
    "        \n",
    "        #####################################################\n",
    "        ## Posterior\n",
    "        #####################################################\n",
    "\n",
    "        self.gp_init_f_posterior_loc_mc = torch.zeros(\n",
    "            (self.num_samples, self.num_cell_types), device=device, dtype=dtype)\n",
    "\n",
    "    @pyro_method\n",
    "    def model(self, xi_mq: torch.Tensor) -> torch.Tensor:\n",
    "        self.set_mode(\"model\")\n",
    "        \n",
    "        # assert that covariates have the same shape as what given to the initializer\n",
    "        assert xi_mq.shape == (self.num_samples, self.covariate_n_dim)\n",
    "        \n",
    "        # sample the inducing points (this happens implicitly in the model() call to gp)\n",
    "        self.gp.set_data(X=xi_mq, y=None)\n",
    "        f_loc_cm, f_var_cm = self.gp.model()\n",
    "                \n",
    "        assert f_loc_cm.shape == (self.num_cell_types, self.num_samples)\n",
    "        assert f_var_cm.shape == (self.num_cell_types, self.num_samples)\n",
    "        \n",
    "        # permute the indices and var -> std\n",
    "        f_loc_mc = f_loc_cm.permute(-1, -2)\n",
    "        f_scale_mc = f_var_cm.sqrt().permute(-1, -2)\n",
    "\n",
    "        with pyro.plate(\"batch\"):\n",
    "            f_mc = pyro.sample(\n",
    "                \"f_mc\",\n",
    "                pyro.distributions.Normal(\n",
    "                    loc=f_loc_mc,\n",
    "                    scale=f_scale_mc).to_event(1))\n",
    "\n",
    "        # finally, apply a softmax to bring the unnormalized cell population (\"f) inside\n",
    "        # the simplex\n",
    "        cell_pop_mc = torch.softmax(f_mc, -1)\n",
    "        assert cell_pop_mc.shape == (self.num_samples, self.num_cell_types)\n",
    "\n",
    "        return cell_pop_mc\n",
    "\n",
    "    @pyro_method\n",
    "    def guide(self, xi_mq: torch.Tensor) -> torch.Tensor:\n",
    "        self.set_mode(\"guide\")\n",
    "\n",
    "        # sample the posterior of the inducing points (happens implicitly inside the guide() call of gp)\n",
    "        self.gp.guide()\n",
    "        \n",
    "        # sample the posterior of the unnormalized cell population (\"f)\n",
    "        f_posterior_loc_mc = pyro.param(\n",
    "            \"f_posterior_loc_mc\",\n",
    "            self.gp_init_f_posterior_loc_mc)\n",
    "        with pyro.plate(\"batch\"):\n",
    "            f_mc = pyro.sample(\n",
    "                \"f_mc\",\n",
    "                pyro.distributions.Delta(v=f_posterior_loc_mc).to_event(1))\n",
    "\n",
    "        # finally, apply a softmax to bring the unnormalized cell population (\"f) inside\n",
    "        # the simplex\n",
    "        cell_pop_mc = torch.softmax(f_mc, -1)\n",
    "        assert cell_pop_mc.shape == (self.num_samples, self.num_cell_types)\n",
    "\n",
    "        return cell_pop_mc\n",
    "    \n",
    "    def calculate_composition_trajectories(\n",
    "            self, dataset, n_intervals=100, return_vals=False\n",
    "        ):\n",
    "        \"\"\"Calculate the composition trajectories\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = VSGPTrajectoryModule(\n",
    "    xi_mq=ebov_dataset.t_m[..., None].contiguous(),\n",
    "    num_cell_types=ebov_dataset.num_cell_types,\n",
    "    init_posterior_global_scale_factor=0.1,\n",
    "    device=ebov_dataset.device,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-confidence",
   "metadata": {},
   "source": [
    "## `time_deconv.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "from torch.distributions import constraints\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from typing import List, Dict\n",
    "import pyro.distributions as dist\n",
    "import anndata\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import math\n",
    "import tqdm\n",
    "import copy\n",
    "from matplotlib.pyplot import cm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scanpy as sc\n",
    "\n",
    "from ternadecov.stats_helpers import *\n",
    "from ternadecov.simulator import *\n",
    "from ternadecov.stats_helpers import *\n",
    "from ternadecov.hypercluster import *\n",
    "from ternadecov.dataset import *\n",
    "from ternadecov.trajectories import *\n",
    "\n",
    "# Indices:\n",
    "# - c cell type\n",
    "# - g genes\n",
    "# - m samples\n",
    "# - k deformation polynomial degree\n",
    "\n",
    "\n",
    "def generate_batch(\n",
    "    dataset: DeconvolutionDataset, device: torch.device, dtype: torch.dtype\n",
    "):\n",
    "\n",
    "    return {\n",
    "        \"x_mg\": dataset.bulk_raw_gex_mg.clone().detach().to(device).type(dtype),\n",
    "        \"t_m\": torch.tensor(dataset.dpi_time_m, device=device, dtype=dtype),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "_TRAJECTORY_MODEL_TYPES = {'polynomial', 'gp'}\n",
    "\n",
    "\n",
    "class TimeRegularizedDeconvolution:\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: DeconvolutionDataset,\n",
    "            device: torch.device,\n",
    "            dtype: torch.dtype,\n",
    "            use_betas: bool = True,\n",
    "            trajectory_model_type: str = 'polynomial',\n",
    "            **kwargs\n",
    "        ):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.use_betas = use_betas\n",
    "\n",
    "        self.init_posterior_global_scale_factor = 0.05\n",
    "\n",
    "        # hyperparameters\n",
    "        self.log_beta_prior_scale = 1.0\n",
    "        self.tau_prior_scale = 1.0\n",
    "        self.log_phi_prior_loc = -5.0\n",
    "        self.log_phi_prior_scale = 1.0\n",
    "\n",
    "        assert trajectory_model_type in _TRAJECTORY_MODEL_TYPES\n",
    "        \n",
    "        if trajectory_model_type == 'polynomial':          \n",
    "            raise NotImplementedError\n",
    "#             self.population_proportion_model = BasicTrajectoryModule(\n",
    "#                 basis_functions=kwargs['basis_functions'],\n",
    "#                 polynomial_degree=kwargs['polynomial_degree'],\n",
    "#                 num_cell_types=self.dataset.num_cell_types,\n",
    "#                 num_samples=self.dataset.num_samples,\n",
    "#                 init_posterior_global_scale_factor=self.init_posterior_global_scale_factor,\n",
    "#                 device=device,\n",
    "#                 dtype=dtype)\n",
    "            \n",
    "        elif trajectory_model_type == 'gp':\n",
    "            self.population_proportion_model = VSGPTrajectoryModule(\n",
    "                xi_mq=self.dataset.t_m[..., None].contiguous(),\n",
    "                num_cell_types=self.dataset.num_cell_types,\n",
    "                init_posterior_global_scale_factor=self.init_posterior_global_scale_factor,\n",
    "                device=device,\n",
    "                dtype=dtype)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        #####################################################\n",
    "        ## Prior\n",
    "        #####################################################\n",
    "\n",
    "        #####################################################\n",
    "        ## Posterior\n",
    "        #####################################################\n",
    "\n",
    "        self.log_beta_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.tau_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.log_phi_posterior_loc = -5.0\n",
    "        self.log_phi_posterior_scale = 0.1 * self.init_posterior_global_scale_factor\n",
    "\n",
    "#         self.dirichlet_alpha_posterior = (\n",
    "#             self.init_posterior_global_scale_factor * np.ones((1,))\n",
    "#         )\n",
    "\n",
    "        # cache useful tensors\n",
    "        self.w_hat_gc = torch.tensor(self.dataset.w_hat_gc, device=device, dtype=dtype)\n",
    "\n",
    "    def model(\n",
    "            self,\n",
    "            x_mg: torch.Tensor,\n",
    "            t_m: torch.Tensor,\n",
    "        ):\n",
    "        \"\"\"Main model\n",
    "\n",
    "        :param x_mg: gene expression\n",
    "        :param t_m: obseration time\n",
    "        \"\"\"\n",
    "\n",
    "        # sample log_phi_g\n",
    "        log_phi_g = pyro.sample(\n",
    "            \"log_phi_g\",\n",
    "            dist.Normal(\n",
    "                loc=self.log_phi_prior_loc\n",
    "                * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype\n",
    "                ),\n",
    "                scale=self.log_phi_prior_scale\n",
    "                * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype\n",
    "                ),\n",
    "            ).to_event(1),\n",
    "        )\n",
    "        assert log_phi_g.shape == (self.dataset.num_genes,)\n",
    "\n",
    "        # sample log_beta_g\n",
    "        log_beta_g = pyro.sample(\n",
    "            \"log_beta_g\",\n",
    "            dist.Normal(\n",
    "                loc=torch.zeros(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype\n",
    "                ),\n",
    "                scale=self.log_beta_prior_scale\n",
    "                * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype\n",
    "                ),\n",
    "            ).to_event(1),\n",
    "        )\n",
    "        assert log_beta_g.shape == (self.dataset.num_genes,)\n",
    "\n",
    "        # calculate useful derived variables\n",
    "        beta_g = log_beta_g.exp()\n",
    "        phi_g = log_phi_g.exp()\n",
    "\n",
    "        # Get normalized w_gc\n",
    "        if self.use_betas:\n",
    "            unnorm_w_gc = self.w_hat_gc * beta_g[:, None]\n",
    "        else:\n",
    "            unnorm_w_gc = self.w_hat_gc\n",
    "\n",
    "        w_gc = unnorm_w_gc / unnorm_w_gc.sum(0)\n",
    "\n",
    "        # get the prior cell populations from the trajectory module\n",
    "        cell_pop_mc = self.population_proportion_model.model(\n",
    "            xi_mq=t_m[..., None].contiguous()\n",
    "        )\n",
    "\n",
    "        # calculate mean gene expression\n",
    "        mu_mg = x_mg.sum(-1)[:, None] * torch.matmul(\n",
    "            cell_pop_mc, w_gc.transpose(-1, -2)\n",
    "        )\n",
    "\n",
    "        with pyro.plate(\"batch\"):\n",
    "            # observe gene expression\n",
    "            # todo: sample specific phi?\n",
    "            pyro.sample(\n",
    "                \"x_mg\",\n",
    "                NegativeBinomialAltParam(mu=mu_mg, phi=phi_g[None, :]).to_event(1),\n",
    "                obs=x_mg,\n",
    "            )\n",
    "\n",
    "    def delta_guide(self, x_mg: torch.Tensor, t_m: torch.Tensor):\n",
    "        \"\"\"Simple delta guide\"\"\"\n",
    "\n",
    "        # variational parameters for log_phi_g\n",
    "        log_phi_posterior_loc_g = pyro.param(\n",
    "            \"log_phi_posterior_loc_g\",\n",
    "            self.log_phi_posterior_loc\n",
    "            * torch.ones(\n",
    "                (self.dataset.num_genes,), device=self.device, dtype=self.dtype\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # variational parameters for log_beta_g\n",
    "        log_beta_posterior_loc_g = pyro.param(\n",
    "            \"log_beta_posterior_loc_g\",\n",
    "            torch.zeros(\n",
    "                (self.dataset.num_genes,), device=self.device, dtype=self.dtype\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # posterior sample statements\n",
    "        log_phi_g = pyro.sample(\n",
    "            \"log_phi_g\", dist.Delta(v=log_phi_posterior_loc_g).to_event(1)\n",
    "        )\n",
    "\n",
    "        log_beta_g = pyro.sample(\n",
    "            \"log_beta_g\", dist.Delta(v=log_beta_posterior_loc_g).to_event(1)\n",
    "        )\n",
    "\n",
    "        # get the posterior cell populations from the trajectory module\n",
    "        cell_pop_mc = self.population_proportion_model.guide(\n",
    "            xi_mq=t_m[..., None].contiguous()\n",
    "        )\n",
    "\n",
    "    def fit_model(\n",
    "            self,\n",
    "            n_iters=3000,\n",
    "            log_frequency=100,\n",
    "            verbose=True,\n",
    "            clear_param_store=True,\n",
    "            keep_param_store_history=True,\n",
    "        ):\n",
    "        if clear_param_store:\n",
    "            pyro.clear_param_store()\n",
    "\n",
    "        ## TODO: bring these out\n",
    "        optim = pyro.optim.Adam({\"lr\": 1e-3})\n",
    "\n",
    "        self.loss_hist = []\n",
    "        self.param_store_hist = []\n",
    "\n",
    "        svi = SVI(\n",
    "            model=self.model, guide=self.delta_guide, optim=optim, loss=Trace_ELBO()\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        for i_iter in range(n_iters):\n",
    "            batch_dict = generate_batch(self.dataset, self.device, self.dtype)\n",
    "\n",
    "            loss = svi.step(**batch_dict)\n",
    "            self.loss_hist.append(loss)\n",
    "\n",
    "            if keep_param_store_history:\n",
    "                param_store = pyro.get_param_store()\n",
    "                self.param_store_hist.append(\n",
    "                    {\n",
    "                        k: v.detach().float().cpu().clone()\n",
    "                        for k, v in param_store.items()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if verbose:\n",
    "                if i_iter % log_frequency == 0:\n",
    "\n",
    "                    print(\n",
    "                        f\"[step: {i_iter}, time: {math.ceil(time.time() - start_time)} s ] loss: {self.loss_hist[-1]:.2f}\"\n",
    "                    )\n",
    "\n",
    "    def plot_loss(self):\n",
    "        \"\"\"Plot the losses during training\"\"\"\n",
    "        fig, ax = matplotlib.pyplot.subplots()\n",
    "        ax.plot(self.loss_hist)\n",
    "        ax.set_title(\"Losses\")\n",
    "        ax.set_xlabel(\"iteration\")\n",
    "        ax.set_ylabel(\"ELBO Loss\")\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def calculate_composition_trajectories(self, n_intervals=100, return_vals=False):\n",
    "        self.population_proportion_model.calculate_composition_trajectories(\n",
    "            self.dataset, n_intervals=100, return_vals=False\n",
    "        )\n",
    "\n",
    "    def get_composition_trajectories(self):\n",
    "        \"\"\"Return the composition trajectories\"\"\"\n",
    "        return self.population_proportion_model.calculated_trajectories\n",
    "\n",
    "    def plot_composition_trajectories(self, show_hypercluster=False):\n",
    "        \"\"\"Plot the composition trajectories\"\"\"\n",
    "\n",
    "        if self.dataset.is_hyperclustered and not show_hypercluster:\n",
    "            fig, ax = matplotlib.pyplot.subplots()\n",
    "            ax.plot(\n",
    "                self.calculated_trajectories[\"true_times_z\"],\n",
    "                self.calculated_trajectories[\"summarized_composition_rt\"].T,\n",
    "            )\n",
    "            ax.set_title(\"Predicted cell proportions\")\n",
    "            ax.set_xlabel(\"Time\")\n",
    "\n",
    "            labels = []\n",
    "\n",
    "            r = self.calculated_trajectories[\"toplevel_cell_map\"]\n",
    "            map_r = {r[k]: k for k in r}\n",
    "            for i in range(len(map_r)):\n",
    "                labels.append(map_r[i])\n",
    "            ax.legend(labels, loc=\"best\", fontsize=\"small\")\n",
    "        else:\n",
    "            fig, ax = matplotlib.pyplot.subplots()\n",
    "            ax.plot(\n",
    "                self.population_proportion_model.calculated_trajectories[\n",
    "                    \"true_times_z\"\n",
    "                ],\n",
    "                self.population_proportion_model.calculated_trajectories[\n",
    "                    \"norm_comp_tc\"\n",
    "                ],\n",
    "            )\n",
    "            ax.set_title(\"Predicted cell proportions\")\n",
    "            ax.set_xlabel(\"Time\")\n",
    "            ax.legend(self.dataset.cell_type_str_list, loc=\"best\", fontsize=\"small\")\n",
    "\n",
    "    def plot_phi_g_distribution(self):\n",
    "        \"\"\"Plot the distribution of phi_g\"\"\"\n",
    "        phi_g = pyro.param(\"log_phi_posterior_loc_g\").clone().detach().exp().cpu()\n",
    "        fig, ax = matplotlib.pyplot.subplots()\n",
    "        ax.hist(phi_g.numpy(), bins=100)\n",
    "        ax.set_xlabel(\"$\\phi_g$\")\n",
    "        ax.set_ylabel(\"Counts\")\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plot_beta_g_distribution(self):\n",
    "        \"\"\"Plot distribution of beta_g\"\"\"\n",
    "\n",
    "        beta_g = pyro.param(\"log_beta_posterior_loc_g\").clone().detach().exp().cpu()\n",
    "        fig, ax = matplotlib.pyplot.subplots()\n",
    "        ax.hist(beta_g.numpy(), bins=100)\n",
    "        ax.set_xlabel(\"$beta_g$\")\n",
    "        ax.set_ylabel(\"Counts\")\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def write_sample_compositions(self, csv_filename, ignore_hypercluster=False):\n",
    "        \"\"\"Write sample composition to csv file\"\"\"\n",
    "\n",
    "        if self.dataset.is_hyperclustered and not ignore_hypercluster:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            self.write_sample_composition_default(csv_filename)\n",
    "\n",
    "    def sample_composition_default(self):\n",
    "        \"\"\"Return the sample composition in a pandas DataFrame\"\"\"\n",
    "\n",
    "        cell_pop_mc = pyro.param(\"cell_pop_posterior_loc_mc\").clone().detach().cpu()\n",
    "        col_sample = []\n",
    "        col_celltype = []\n",
    "        col_proportion = []\n",
    "        for i_0 in range(cell_pop_mc.shape[0]):\n",
    "            for i_1 in range(cell_pop_mc.shape[1]):\n",
    "                col_sample.append(self.dataset.bulk_sample_names[i_0])\n",
    "                col_celltype.append(self.dataset.cell_type_str_list[i_1])\n",
    "                col_proportion.append(cell_pop_mc[i_0, i_1].item())\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"col_sample\": col_sample,\n",
    "                \"col_celltype\": col_celltype,\n",
    "                \"col_proportion\": col_proportion,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def write_sample_composition_default(self, csv_filename):\n",
    "        \"\"\"Write sample composition proportions to csv file\n",
    "        \n",
    "        :param csv_filename: filename of csv file to write to\n",
    "        \"\"\"\n",
    "\n",
    "        composition_df = self.sample_composition_default()\n",
    "        composition_df.to_csv(csv_filename)\n",
    "\n",
    "    def plot_sample_compositions_scatter(\n",
    "        self, figsize=(16, 9), ignore_hypercluster=False\n",
    "    ):\n",
    "        \"\"\"Plot a facetted scatter plot of the individual sample compositions\n",
    "\n",
    "        :param figsize: tuple of size 2 with figure size information\n",
    "        \"\"\"\n",
    "        if self.dataset.is_hyperclustered and not ignore_hypercluster:\n",
    "            self.plot_sample_compositions_scatter_hyperclustered(figsize=figsize)\n",
    "        else:\n",
    "            self.plot_sample_compositions_scatter_default(figsize=figsize)\n",
    "\n",
    "    def plot_sample_compositions_scatter_default(self, figsize):\n",
    "        \"\"\"Plot a facetted scatter plot of the individual sample compositions for regular processing\n",
    "\n",
    "        :param figsize: tuple of size 2 with figure size information\n",
    "        \"\"\"\n",
    "        t_m = self.dataset.t_m.clone().detach().cpu()\n",
    "        cell_pop_mc = pyro.param(\"cell_pop_posterior_loc_mc\").clone().detach().cpu()\n",
    "        sort_order = torch.argsort(self.dataset.t_m)\n",
    "\n",
    "        n_cell_types = cell_pop_mc.shape[1]\n",
    "\n",
    "        n_rows = math.ceil(math.sqrt(n_cell_types))\n",
    "        n_cols = math.ceil(n_cell_types / n_rows)\n",
    "\n",
    "        fig, ax = matplotlib.pyplot.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "        for i in range(cell_pop_mc.shape[1]):\n",
    "            r_i = int(i // n_rows)\n",
    "            c_i = int(i % n_rows)\n",
    "\n",
    "            ax[c_i, r_i].scatter(\n",
    "                t_m[sort_order] * self.dataset.time_range + self.dataset.time_min,\n",
    "                cell_pop_mc[sort_order, i].clone().detach().cpu(),\n",
    "                color=cm.tab10(i),\n",
    "            )\n",
    "            ax[c_i, r_i].set_title(self.dataset.cell_type_str_list[i])\n",
    "\n",
    "        matplotlib.pyplot.tight_layout()\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plot_sample_compositions_scatter_hyperclustered(self, figsize):\n",
    "        \"\"\"Plot a facetted scatter plot of the individual sample compositions for hyperclustered processing\n",
    "\n",
    "        :param figsize: tuple of size 2 with figure size information\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.dataset.is_hyperclustered\n",
    "\n",
    "        t_m = self.dataset.t_m.clone().detach().cpu()\n",
    "        cell_pop_mc = pyro.param(\"cell_pop_posterior_loc_mc\").clone().detach().cpu()\n",
    "        sort_order = torch.argsort(self.dataset.t_m)\n",
    "\n",
    "        ## Summarise cell_pop_mc to the high-level clusters\n",
    "        n_top_level_clusters = len(\n",
    "            set(self.dataset.hypercluster_results[\"cluster_map\"].values())\n",
    "        )\n",
    "        n_low_level_clusters = len(\n",
    "            set(self.dataset.hypercluster_results[\"cluster_map\"].keys())\n",
    "        )\n",
    "        # k is index for  highlevel clusters\n",
    "        cell_pop_summarized_mk = torch.zeros(\n",
    "            (cell_pop_mc.shape[0], n_top_level_clusters)\n",
    "        )\n",
    "\n",
    "        # Low level cluster names\n",
    "        low_cell_type_str_list = self.dataset.cell_type_str_list\n",
    "        toplevel_cell_map = self.calculated_trajectories[\"toplevel_cell_map\"]\n",
    "        high_cell_type_str_list = list(toplevel_cell_map.keys())\n",
    "        low_to_high_clustermap = self.dataset.hypercluster_results[\"cluster_map\"]\n",
    "\n",
    "        index = torch.zeros((n_low_level_clusters,), dtype=torch.int64)\n",
    "\n",
    "        for i_llc in range(n_low_level_clusters):\n",
    "            llc_name = low_cell_type_str_list[i_llc]\n",
    "            hlc_name = low_to_high_clustermap[llc_name]\n",
    "            i_hlcc = high_cell_type_str_list.index(hlc_name)\n",
    "            index[i_llc] = i_hlcc\n",
    "\n",
    "        cell_pop_summarized_mk.index_add_(1, index, cell_pop_mc)\n",
    "\n",
    "        print(f\"cell_pop_summarized_mk shape: {cell_pop_summarized_mk.shape}\")\n",
    "\n",
    "        n_cell_types = cell_pop_summarized_mk.shape[1]\n",
    "\n",
    "        n_rows = math.ceil(math.sqrt(n_cell_types))\n",
    "        n_cols = math.ceil(n_cell_types / n_rows)\n",
    "\n",
    "        fig, ax = matplotlib.pyplot.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "        for i in range(cell_pop_summarized_mk.shape[1]):\n",
    "            r_i = int(i // n_rows)\n",
    "            c_i = int(i % n_rows)\n",
    "\n",
    "            ax[c_i, r_i].scatter(\n",
    "                t_m[sort_order] * self.dataset.time_range + self.dataset.time_min,\n",
    "                cell_pop_summarized_mk[sort_order, i].clone().detach().cpu(),\n",
    "                color=cm.tab10(i),\n",
    "            )\n",
    "\n",
    "            ax[c_i, r_i].set_title(high_cell_type_str_list[i])\n",
    "\n",
    "        matplotlib.pyplot.tight_layout()\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plot_sample_compositions_boxplot(self, figsize=(16, 9)):\n",
    "        figsize = (16, 9)\n",
    "\n",
    "        t_m = self.dataset.t_m.clone().detach().cpu()\n",
    "        cell_pop = pyro.param(\"cell_pop_posterior_loc_mc\").clone().detach().cpu()\n",
    "        sort_order = torch.argsort(self.dataset.t_m)\n",
    "\n",
    "        n_cell_types = cell_pop.shape[1]\n",
    "\n",
    "        n_rows = math.ceil(math.sqrt(n_cell_types))\n",
    "        n_cols = math.ceil(n_cell_types / n_rows)\n",
    "\n",
    "        fig, ax = matplotlib.pyplot.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "        for i in range(cell_pop.shape[1]):\n",
    "            r_i = int(i // n_rows)\n",
    "            c_i = int(i % n_rows)\n",
    "\n",
    "            t = t_m[sort_order] * self.dataset.time_range + self.dataset.time_min\n",
    "            prop = cell_pop[sort_order, i].clone().detach().cpu()\n",
    "            labels = self.dataset.cell_type_str_list[i]\n",
    "\n",
    "            df1 = pd.DataFrame({\"time\": t, \"proportion\": prop,})\n",
    "\n",
    "            sns.boxplot(\n",
    "                x=\"time\", y=\"proportion\", data=df1, ax=ax[c_i, r_i], color=cm.tab10(i)\n",
    "            )\n",
    "            ax[c_i, r_i].set_title(self.dataset.cell_type_str_list[i])\n",
    "\n",
    "        matplotlib.pyplot.tight_layout()\n",
    "\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-equation",
   "metadata": {},
   "source": [
    "# Run Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv = TimeRegularizedDeconvolution(\n",
    "    dataset=ebov_dataset,\n",
    "    trajectory_model_type='gp',\n",
    "    device=device,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.fit_model(n_iters=n_iters, verbose=True, log_frequency=100, clear_param_store=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-thesis",
   "metadata": {},
   "source": [
    "# Examine Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loses\n",
    "pseudo_time_reg_deconv.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    traj = pseudo_time_reg_deconv.population_proportion_model\n",
    "    xi_new_nq = torch.linspace(0., 1., 1000, device=device, dtype=dtype)[..., None]\n",
    "    f_new_loc_cn, f_new_var_cn = traj.gp.forward(xi_new_nq, full_cov=False)\n",
    "    f_new_scale_cn = f_new_var_cn.sqrt()\n",
    "    pi_new_loc_cn = torch.softmax(f_new_loc_cn, dim=0)\n",
    "\n",
    "plt.plot(pi_new_loc_cn.cpu().numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-disability",
   "metadata": {},
   "source": [
    "## Plotting demo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "with torch.no_grad():\n",
    "    traj = pseudo_time_reg_deconv.population_proportion_model\n",
    "    xi_new_nq = torch.linspace(0., 1., 1000, device=device, dtype=dtype)[..., None]\n",
    "    f_new_loc_cn, f_new_var_cn = traj.gp.forward(xi_new_nq, full_cov=False)\n",
    "    f_new_scale_cn = f_new_var_cn.sqrt()\n",
    "    f_new_sampled_scn = torch.distributions.Normal(f_new_loc_cn, f_new_scale_cn).sample([n_samples])\n",
    "    pi_new_sampled_scn = torch.softmax(f_new_sampled_scn, dim=1)\n",
    "    pi_new_loc_cn = torch.softmax(f_new_loc_cn, dim=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# plot the mean trajectory\n",
    "ax.plot(\n",
    "    xi_new_nq.cpu().numpy(),\n",
    "    pi_new_loc_cn.cpu().numpy().T);\n",
    "\n",
    "# plot samples\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "for i_cell_type in range(pi_new_loc_cn.shape[0]):\n",
    "    color = colors[i_cell_type]\n",
    "    ax.scatter(\n",
    "        x=xi_new_nq.expand((n_samples,) + xi_new_nq.shape).cpu().numpy().flatten(),\n",
    "        y=pi_new_sampled_scn[:, i_cell_type, :].cpu().numpy().flatten(),\n",
    "        c=color,\n",
    "        alpha=0.1,\n",
    "        s=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and plot the composition trajectories\n",
    "pseudo_time_reg_deconv.calculate_composition_trajectories(n_intervals = 1000)\n",
    "pseudo_time_reg_deconv.plot_composition_trajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the per-gene dispersions\n",
    "pseudo_time_reg_deconv.plot_phi_g_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the gene capture coefficients\n",
    "pseudo_time_reg_deconv.plot_beta_g_distribution()\n",
    "matplotlib.pyplot.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the alphas\n",
    "\n",
    "alphas = list(pseudo_time_reg_deconv.param_store_hist[i]['dirichlet_alpha'] for i in range(len(pseudo_time_reg_deconv.param_store_hist)))\n",
    "\n",
    "fig, ax = matplotlib.pyplot.subplots(1,2)\n",
    "\n",
    "ax[0].plot(alphas)\n",
    "ax[0].set_title(r'Dirichlet $ \\alpha $ Values')\n",
    "\n",
    "ax[1].plot(alphas[-500:])\n",
    "ax[1].set_title(r'Last few Dirichlet $ \\alpha $ Values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.plot_sample_compositions_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.plot_sample_compositions_boxplot()\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-jumping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
