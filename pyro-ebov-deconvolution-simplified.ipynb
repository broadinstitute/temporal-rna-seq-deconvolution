{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hearing-render",
   "metadata": {},
   "source": [
    "## Ebola infected Macaque Sample Composition Trajectory Identification\n",
    "\n",
    "```\n",
    "Indices:\n",
    "\n",
    "- c cell type\n",
    "- g genes\n",
    "- m samples\n",
    "- k deformation polynomial degree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pylab as plt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "from typing import Dict\n",
    "from pyro.distributions.torch_distribution import TorchDistribution, TorchDistributionMixin\n",
    "from torch.distributions.utils import probs_to_logits, logits_to_probs, broadcast_all, lazy_property\n",
    "from torch.distributions import constraints\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from typing import List, Dict\n",
    "from boltons.cacheutils import cachedproperty\n",
    "from pyro.distributions.torch_distribution import TorchDistribution, TorchDistributionMixin\n",
    "from torch.distributions.utils import probs_to_logits, logits_to_probs, broadcast_all, lazy_property\n",
    "from torch.distributions import constraints\n",
    "from numbers import Number\n",
    "import pyro.distributions as dist\n",
    "import anndata\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, SplineTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-reviewer",
   "metadata": {},
   "source": [
    "## Parameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float32\n",
    "dtype_np = np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-nylon",
   "metadata": {},
   "source": [
    "## Helper functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import legendre\n",
    "\n",
    "def legendre_coefficient_mat(k_max, epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Return the coefficient matrix of legendre polynomials.\n",
    "    \n",
    "    :param k_max: legenre polynomial max degree\n",
    "    :param epsilon: minimum coefficient value\n",
    "    \"\"\"\n",
    "    \n",
    "    k_max_internal = k_max + 1\n",
    "    X_kl = torch.zeros(k_max_internal, k_max_internal)\n",
    "    for i in range(0, k_max_internal):\n",
    "        terms = list(legendre(i))\n",
    "        terms.reverse()\n",
    "        X_kl[i,:i+1] = torch.tensor(terms,dtype=dtype)\n",
    "    X_kl = torch.where(X_kl.abs() < epsilon, torch.zeros_like(X_kl), X_kl)\n",
    "    return X_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegativeBinomialAltParam(mu, phi):\n",
    "    \"\"\"\n",
    "    Creates a negative binomial distribution.\n",
    "    \n",
    "    Args:\n",
    "        mu (Number, Tensor): mean (must be strictly positive)\n",
    "        phi (Number, Tensor): overdispersion (must be strictly positive)\n",
    "    \"\"\"\n",
    "    return dist.GammaPoisson(\n",
    "            concentration= 1 /phi ,\n",
    "            rate=1 / (mu * phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvolutionDataset:\n",
    "    \"\"\"This class represents a bulk and single-cell dataset to be deconvolved in tandem\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            sc_anndata: anndata.AnnData,\n",
    "            sc_celltype_col: str,\n",
    "            bulk_anndata: anndata.AnnData,\n",
    "            bulk_time_col: str,\n",
    "            dtype_np: np.dtype,\n",
    "            feature_selection_method: str = \"common\"):\n",
    "\n",
    "        self.dtype_np = dtype_np\n",
    "        self.sc_celltype_col = sc_celltype_col\n",
    "        self.bul_time_col = bulk_time_col\n",
    "\n",
    "        self.selected_genes = ()\n",
    "        #self.bulk_anndata = bulk_anndata\n",
    "        #self.sc_anndata = sc_anndata\n",
    "        \n",
    "        # Select common genes and subset/order anndata objects\n",
    "        selected_genes = self.__select_features(bulk_anndata, sc_anndata, feature_selection_method = feature_selection_method)\n",
    "        \n",
    "        \n",
    "        self.bulk_anndata = bulk_anndata[:,selected_genes]\n",
    "        self.sc_anndata = sc_anndata[:,selected_genes]\n",
    "        \n",
    "        self.num_genes = len(selected_genes)\n",
    "        \n",
    "        # TODO: Issue warning if too many genes removed\n",
    "        \n",
    "        # Pre-process time values and save inverse function\n",
    "        self.dpi_time_original_m = self.bulk_anndata.obs[bulk_time_col].values.astype(dtype_np)\n",
    "        self.time_min = np.min(self.dpi_time_original_m)\n",
    "        self.time_range = np.max(self.dpi_time_original_m) - self.time_min\n",
    "        self.dpi_time_m = (self.dpi_time_original_m - self.time_min) / self.time_range\n",
    "        \n",
    "        \n",
    "    def __select_features(self, bulk_anndata, sc_anndata, feature_selection_method, dispersion_cutoff = 5):\n",
    "        print(f'feature selection method: {feature_selection_method}')\n",
    "        \n",
    "        if feature_selection_method == \"common\":\n",
    "            self.selected_genes = list(set(bulk_anndata.var.index).intersection(set(sc_anndata.var.index)))\n",
    "        elif feature_selection_method == 'overdispersed_bulk':\n",
    "            x_train = np.log(bulk_anndata.X.mean(0)+1) # log_mu_g\n",
    "            y_train = np.log(bulk_anndata.X.var(0)+1) # log_sigma_g\n",
    "            \n",
    "            X_train = x_train[:, np.newaxis]\n",
    "            degree = 3\n",
    "            model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=1e-3))\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_train)\n",
    "            \n",
    "            # Select Genes\n",
    "            sel_over = (y_train - y_pred > 0.0) & (y_train > dispersion_cutoff)\n",
    "            self.selected_genes = list(bulk_anndata.var.index[sel_over])\n",
    "            \n",
    "        elif feature_selection_method == 'overdispersed_bulk_and_high_sc':\n",
    "            # Fit polynomial degree\n",
    "            polynomial_degree = 2\n",
    "            sc_cutoff = 2 # log scale\n",
    "            \n",
    "            # Select overdispersed in bulk\n",
    "            x_train = np.log(bulk_anndata.X.mean(0)+1) # log_mu_g\n",
    "            y_train = np.log(bulk_anndata.X.var(0)+1) # log_sigma_g\n",
    "            \n",
    "            X_train = x_train[:, np.newaxis]\n",
    "            \n",
    "            model = make_pipeline(PolynomialFeatures(polynomial_degree), Ridge(alpha=1e-3))\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_train)\n",
    "            \n",
    "            # Select Genes\n",
    "            sel_over_bulk = (y_train - y_pred > 0.0) & (y_train > dispersion_cutoff)\n",
    "            selected_genes_bulk = set(bulk_anndata.var.index[sel_over_bulk])\n",
    "            \n",
    "            # Select highly-expressed in single-cell\n",
    "            \n",
    "            selected_genes_sc = set(sc_anndata.var.index[np.log(sc_anndata.X.sum(0)+1) > sc_cutoff])\n",
    "            \n",
    "            self.selected_genes = list(selected_genes_bulk.intersection(selected_genes_sc))\n",
    "            \n",
    "\n",
    "        print(f'selected {len(self.selected_genes)} genes')\n",
    "            \n",
    "        return self.selected_genes\n",
    "\n",
    "# Some plotting code\n",
    "#             sel_under = (y_train - y_pred < 0.0 ) | (y_train <= 7)\n",
    "#             lw = 1e-4\n",
    "#             fig, ax = matplotlib.pyplot.subplots()\n",
    "#             ax.scatter(x_train[sel_under], y_train[sel_under], linewidth=lw, label=\"ground truth\", c='black', marker='.', s=2)\n",
    "#             ax.scatter(x_train[sel_over], y_train[sel_over], linewidth=lw, label=\"ground truth\", c='orange', marker='.', s=2)\n",
    "#             # order for the line\n",
    "#             o = np.argsort(X_train.T)[0]\n",
    "#             matplotlib.pyplot.plot(X_train[o,:], y_pred[o], c='red')\n",
    "\n",
    "        \n",
    "    @cachedproperty\n",
    "    def cell_type_str_list(self) -> List[str]:\n",
    "        return sorted(list(set(self.sc_anndata.obs[self.sc_celltype_col])))\n",
    "    \n",
    "    @cachedproperty\n",
    "    def cell_type_str_to_index_map(self) -> Dict[str, int]:\n",
    "        return {cell_type_str: index for index, cell_type_str in enumerate(self.cell_type_str_list)}        \n",
    "    \n",
    "    @cachedproperty\n",
    "    def num_cell_types(self) -> int:\n",
    "        return len(self.cell_type_str_list)\n",
    "    \n",
    "    @cachedproperty\n",
    "    def w_hat_gc(self) -> np.ndarray:\n",
    "        \"\"\"Calculate the estimate cell profiles\"\"\"\n",
    "        w_hat_gc = np.zeros((self.num_genes, self.num_cell_types))\n",
    "        for cell_type_str in self.cell_type_str_list:\n",
    "            i_cell_type = self.cell_type_str_to_index_map[cell_type_str]\n",
    "            mask_j = self.sc_anndata.obs[self.sc_celltype_col].values == cell_type_str\n",
    "            w_hat_gc[:, i_cell_type] = np.sum(self.sc_anndata.X[mask_j,:], axis=-2)\n",
    "            w_hat_gc[:, i_cell_type] = w_hat_gc[:, i_cell_type] / np.sum(w_hat_gc[:, i_cell_type])\n",
    "        return w_hat_gc\n",
    "    \n",
    "    @cachedproperty\n",
    "    def bulk_raw_gex_mg(self) -> torch.tensor:\n",
    "        return torch.tensor(self.bulk_anndata.X, device=device, dtype=dtype)\n",
    "    \n",
    "    @cachedproperty\n",
    "    def t_m(self) -> torch.tensor:\n",
    "        return torch.tensor(self.dpi_time_m, device=device, dtype=dtype)\n",
    "    \n",
    "#     def num_bulk_samples() -> int:\n",
    "#         return self.bulk_anndata.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    dataset: DeconvolutionDataset,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype):\n",
    "    \n",
    "    return {\n",
    "        \"x_mg\": torch.tensor(dataset.bulk_raw_gex_mg, device=device, dtype=dtype),\n",
    "        \"t_m\": torch.tensor(dataset.dpi_time_m, device=device, dtype=dtype),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-chain",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRegularizedDeconvolution:\n",
    "    \n",
    "    def  __init__(\n",
    "            self,\n",
    "            dataset: DeconvolutionDataset,\n",
    "            basis_functions: str,\n",
    "            polynomial_degree: int,\n",
    "            device: device,\n",
    "            dtype: dtype,\n",
    "            use_betas:bool = True):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.polynomial_degree = polynomial_degree\n",
    "        self.basis_functions = basis_functions\n",
    "        self.use_betas = use_betas\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.log_beta_prior_scale = 1.0\n",
    "        self.log_r_prior_scale = 1.0\n",
    "        self.tau_prior_scale = 1.0\n",
    "        self.log_phi_prior_loc = -5.0\n",
    "        self.log_phi_prior_scale = 1.0\n",
    "        \n",
    "        self.unnorm_cell_pop_base_prior_loc_c = np.zeros((self.dataset.num_cell_types,))\n",
    "        self.unnorm_cell_pop_base_prior_scale_c = np.ones((self.dataset.num_cell_types,))\n",
    "\n",
    "        # dist of coefficients of population deformation polynomial\n",
    "        self.unnorm_cell_pop_deform_prior_loc_ck = np.zeros((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "        self.unnorm_cell_pop_deform_prior_scale_ck = np.ones((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "\n",
    "\n",
    "        self.init_posterior_global_scale_factor = 0.05\n",
    "        \n",
    "        self.log_beta_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.log_r_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.tau_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.log_phi_posterior_loc = -5.0\n",
    "        self.log_phi_posterior_scale = 0.1 * self.init_posterior_global_scale_factor\n",
    "        \n",
    "        \n",
    "        self.unnorm_cell_pop_base_posterior_loc_c = np.zeros((self.dataset.num_cell_types,))\n",
    "        self.unnorm_cell_pop_base_posterior_scale_c = self.init_posterior_global_scale_factor * np.ones((self.dataset.num_cell_types,))\n",
    "        \n",
    "        self.unnorm_cell_pop_deform_posterior_loc_ck = np.zeros((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "        self.unnorm_cell_pop_deform_posterior_scale_ck = self.init_posterior_global_scale_factor *  np.ones((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "        \n",
    "        # cache useful tensors\n",
    "        self.w_hat_gc = torch.tensor(self.dataset.w_hat_gc, device=device, dtype=dtype)\n",
    "        \n",
    "    def model(\n",
    "        self,\n",
    "        x_mg: torch.Tensor,\n",
    "        t_m: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"Main model\n",
    "        \n",
    "        :param x_mg: gene expression\n",
    "        :param t_m: obseration time\n",
    "        \"\"\"\n",
    "        \n",
    "        # sample log_phi_g\n",
    "        log_phi_g = pyro.sample(\n",
    "            'log_phi_g',\n",
    "            dist.Normal(\n",
    "                loc=self.log_phi_prior_loc * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype),\n",
    "                scale=self.log_phi_prior_scale * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype)).to_event(1))\n",
    "        \n",
    "        # sample log_beta_g\n",
    "        log_beta_g = pyro.sample(\n",
    "            'log_beta_g',\n",
    "            dist.Normal(\n",
    "                loc=torch.zeros(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype),\n",
    "                scale=self.log_beta_prior_scale * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype)).to_event(1))\n",
    "        \n",
    "        # sample unnorm_cell_pop_base_c\n",
    "        unnorm_cell_pop_base_c = pyro.sample(\n",
    "            'unnorm_cell_pop_base_c',\n",
    "            dist.Normal(\n",
    "                loc=torch.tensor(\n",
    "                    self.unnorm_cell_pop_base_prior_loc_c,\n",
    "                    device=self.device, dtype=self.dtype),\n",
    "                scale=torch.tensor(\n",
    "                    self.unnorm_cell_pop_base_prior_scale_c,\n",
    "                    device=self.device, dtype=self.dtype)).to_event(1))\n",
    "        \n",
    "        # Deformation scale is a learnable parameter now\n",
    "        unnorm_cell_pop_deform_prior_scale_ck = pyro.param(\"unnorm_cell_pop_deform_prior_scale_ck\",\n",
    "                        torch.tensor(self.unnorm_cell_pop_deform_prior_scale_ck, \n",
    "                        device=self.device,\n",
    "                        dtype=self.dtype),                          \n",
    "                        constraint = constraints.positive) \n",
    "        \n",
    "        unnorm_cell_pop_deform_ck = pyro.sample(\n",
    "            'unnorm_cell_pop_deform_ck',\n",
    "            dist.Normal(\n",
    "                loc=torch.tensor(\n",
    "                    self.unnorm_cell_pop_deform_prior_loc_ck,\n",
    "                    device=self.device, dtype=self.dtype),\n",
    "                scale=unnorm_cell_pop_deform_prior_scale_ck).to_event(2))\n",
    "        \n",
    "        # calculate useful derived variables\n",
    "        beta_g = log_beta_g.exp()\n",
    "        phi_g = log_phi_g.exp()\n",
    "        \n",
    "        # Get normalized w_gc\n",
    "        if self.use_betas:\n",
    "            unnorm_w_gc = self.w_hat_gc * beta_g[:, None]\n",
    "        else:\n",
    "            unnorm_w_gc = self.w_hat_gc\n",
    "            \n",
    "        w_gc = unnorm_w_gc / unnorm_w_gc.sum(0)\n",
    "\n",
    "        if self.basis_functions == 'polynomial':\n",
    "            tau_km = torch.pow(t_m[None,:], torch.arange(1,self.polynomial_degree + 1, device=self.device)[:,None])  \n",
    "            deformation_mc = torch.matmul(unnorm_cell_pop_deform_ck, tau_km).transpose(-1,-2)\n",
    "        elif self.basis_functions == 'legendre':\n",
    "            # l -- power of the term of the legrenre polynomial\n",
    "            t_m_prime = 2 * t_m - 1  # discrete times in (-1,1)\n",
    "            t_lm = torch.pow(t_m_prime[None,:], torch.arange(0,self.polynomial_degree + 1, device=self.device)[:,None])\n",
    "            c_kl = legendre_coefficient_mat(self.polynomial_degree)[1:,].to(device) # drop constant term\n",
    "            intermediate_legenre_vals_km = torch.matmul(c_kl, t_lm)\n",
    "            deformation_mc = torch.matmul(unnorm_cell_pop_deform_ck, intermediate_legenre_vals_km).transpose(-1,-2)\n",
    "            \n",
    "        # TODO: add a sample specific proportion sampling from the corresponding time point\n",
    "        \n",
    "        cell_pop_unnorm_mc = unnorm_cell_pop_base_c[None,:] + deformation_mc\n",
    "            \n",
    "        # Normalize the cell proportions\n",
    "        cell_pop_mc = torch.nn.functional.softmax(cell_pop_unnorm_mc, dim=-1)\n",
    "        \n",
    "        # calculate mean gene expression\n",
    "        mu_mg = x_mg.sum(-1)[:, None] * torch.matmul(cell_pop_mc, w_gc.transpose(-1, -2))\n",
    "\n",
    "        with pyro.plate('batch'):\n",
    "            # observe gene expression\n",
    "            pyro.sample(\n",
    "                \"x_mg\",\n",
    "                NegativeBinomialAltParam(\n",
    "                    mu=mu_mg,\n",
    "                    phi=phi_g[None, :]).to_event(1), # sample specific phi?\n",
    "                obs=x_mg)\n",
    "\n",
    "    def delta_guide(\n",
    "            self,\n",
    "            x_mg: torch.Tensor,\n",
    "            t_m: torch.Tensor):\n",
    "        \"\"\"Simple delta guide\"\"\"\n",
    "        \n",
    "        # variational parameters for log_phi_g\n",
    "        log_phi_posterior_loc_g = pyro.param(\n",
    "            \"log_phi_posterior_loc_g\",\n",
    "            self.log_phi_posterior_loc * torch.ones(\n",
    "                (self.dataset.num_genes,), device=self.device, dtype=self.dtype))\n",
    "\n",
    "        # variational parameters for log_beta_g\n",
    "        log_beta_posterior_loc_g = pyro.param(\n",
    "            \"log_beta_posterior_loc_g\",\n",
    "            torch.zeros(\n",
    "                (self.dataset.num_genes,), device=self.device, dtype=self.dtype))        \n",
    "        \n",
    "        # variational parameters for unnorm_cell_pop_base_c (\"B_c\")\n",
    "        unnorm_cell_pop_base_posterior_loc_c = pyro.param(\n",
    "            \"unnorm_cell_pop_base_posterior_loc_c\",\n",
    "            torch.tensor(\n",
    "                self.unnorm_cell_pop_base_posterior_loc_c,\n",
    "                device=self.device, dtype=self.dtype))\n",
    "        \n",
    "        # variational parameters for unnorm_cell_pop_deform_c (\"R_c\")\n",
    "        unnorm_cell_pop_deform_posterior_loc_ck = pyro.param(\n",
    "            \"unnorm_cell_pop_deform_posterior_loc_ck\",\n",
    "            torch.tensor(\n",
    "                self.unnorm_cell_pop_deform_posterior_loc_ck,\n",
    "                device=self.device, dtype=self.dtype))\n",
    "        \n",
    "        # posterior sample statements\n",
    "        log_phi_g = pyro.sample(\n",
    "            \"log_phi_g\",\n",
    "            dist.Delta(\n",
    "                v=log_phi_posterior_loc_g).to_event(1))\n",
    "\n",
    "        log_beta_g = pyro.sample(\n",
    "            \"log_beta_g\",\n",
    "            dist.Delta(\n",
    "                v=log_beta_posterior_loc_g).to_event(1))\n",
    "\n",
    "        unnorm_cell_pop_base_c = pyro.sample(\n",
    "            'unnorm_cell_pop_base_c',\n",
    "            dist.Delta(\n",
    "                v=unnorm_cell_pop_base_posterior_loc_c).to_event(1))\n",
    "        \n",
    "        unnorm_cell_pop_deform_ck = pyro.sample(\n",
    "            'unnorm_cell_pop_deform_ck',\n",
    "            dist.Delta(v=unnorm_cell_pop_deform_posterior_loc_ck).to_event(2))\n",
    "        \n",
    "        \n",
    "    def fit_model(self, n_iters = 3000, log_frequency = 100, verbose = True, clear_param_store = True):\n",
    "        if clear_param_store:\n",
    "            pyro.clear_param_store()\n",
    "            \n",
    "        ## TODO: bring these out\n",
    "        optim = pyro.optim.Adam({'lr': 1e-3})\n",
    "        \n",
    "        self.loss_hist = []\n",
    "        \n",
    "        svi = SVI(\n",
    "            model=self.model,\n",
    "            guide=self.delta_guide,\n",
    "            optim=optim,\n",
    "            loss=Trace_ELBO())\n",
    "        \n",
    "        for i_iter in range(n_iters):\n",
    "            batch_dict = generate_batch(self.dataset, device, dtype)\n",
    "            loss = svi.step(**batch_dict)\n",
    "            self.loss_hist.append(loss)\n",
    "            if verbose:\n",
    "                if i_iter % log_frequency == 0:\n",
    "                    print(f\"[iteration: {i_iter}]   loss: {self.loss_hist[-1]:.2f}\")\n",
    "                \n",
    "    def plot_loss(self):\n",
    "        \"\"\"Plot the losses during training\"\"\"\n",
    "        fig, ax = matplotlib.pyplot.subplots()\n",
    "        ax.plot(self.loss_hist)\n",
    "        ax.set_title('Losses')\n",
    "        ax.set_xlabel('iteration')\n",
    "        ax.set_ylabel('ELBO Loss')\n",
    "        \n",
    "        return ax\n",
    "    \n",
    "    \n",
    "    def calculate_composition_trajectories(self, n_intervals = 100, return_vals = False):\n",
    "        \"\"\"Calculate the composition trajectories\"\"\"\n",
    "        \n",
    "        # calculate true times\n",
    "        if self.basis_functions == 'polynomial':\n",
    "            time_step = 1 / n_intervals\n",
    "            \n",
    "            times_z = torch.arange(0,1,time_step)\n",
    "        \n",
    "            # Take time to appropriate exponent\n",
    "            times_zk = torch.pow(times_z[:,None], torch.arange(1,self.polynomial_degree + 1,))\n",
    "\n",
    "            # get the trained params\n",
    "            base_composition_post_c = pyro.param(\"unnorm_cell_pop_base_posterior_loc_c\").detach().cpu()\n",
    "            delta_composition_post_ck = pyro.param(\"unnorm_cell_pop_deform_posterior_loc_ck\").detach().cpu()\n",
    "\n",
    "            # Calculate the deltas for each time point\n",
    "            delta_cz = torch.matmul(delta_composition_post_ck, times_zk.transpose(-1,-2))\n",
    "\n",
    "            # normalize\n",
    "            norm_comp_t = torch.nn.functional.softmax(base_composition_post_c[:,None] + delta_cz, dim=0).numpy().T\n",
    "            \n",
    "            true_times_z = times_z * self.dataset.time_range + self.dataset.time_min\n",
    "        elif self.basis_functions == 'legendre':\n",
    "            time_step = 2 / n_intervals\n",
    "            \n",
    "            times_z = torch.arange(-1,1,time_step)\n",
    "\n",
    "            # Take time to appropriate exponent\n",
    "            times_zk = torch.pow(times_z[:,None], torch.arange(1,self.polynomial_degree + 1,))\n",
    "\n",
    "            # get the trained params\n",
    "            base_composition_post_c = pyro.param(\"unnorm_cell_pop_base_posterior_loc_c\").detach().cpu()\n",
    "            delta_composition_post_ck = pyro.param(\"unnorm_cell_pop_deform_posterior_loc_ck\").detach().cpu()\n",
    "\n",
    "            # Calculate the deltas for each time point\n",
    "            delta_cz = torch.matmul(delta_composition_post_ck, times_zk.transpose(-1,-2))\n",
    "\n",
    "            # normalize\n",
    "            norm_comp_t = torch.nn.functional.softmax(base_composition_post_c[:,None] + delta_cz, dim=0).numpy().T\n",
    "            \n",
    "            true_times_z = ((times_z+1)/2) * self.dataset.time_range + self.dataset.time_min\n",
    "        \n",
    "        self.calculated_trajectories = {\n",
    "            \"times_z\": times_z.numpy(), \n",
    "            \"true_times_z\": true_times_z,\n",
    "            \"norm_comp_t\": norm_comp_t\n",
    "        }\n",
    "        \n",
    "        if return_vals:\n",
    "            return self.calculated_trajectories\n",
    "    \n",
    "    \n",
    "    def get_composition_trajectories(self):\n",
    "        \"\"\"Return the composition trajectories\"\"\"\n",
    "        return self.calculated_trajectories\n",
    "    \n",
    "    \n",
    "    def plot_composition_trajectories(self):\n",
    "        \"\"\"Plot the composition trajectories\"\"\"\n",
    "        fig, ax  = matplotlib.pyplot.subplots()\n",
    "        ax.plot(self.calculated_trajectories[\"true_times_z\"], self.calculated_trajectories[\"norm_comp_t\"])\n",
    "        ax.set_title(\"Predicted cell proportions\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.legend(ebov_dataset.cell_type_str_list,loc='best',fontsize='small')\n",
    "        \n",
    "    def plot_phi_g_distribution(self):\n",
    "        \"\"\"Plot the distribution of phi_g\"\"\"\n",
    "        phi_g = pyro.param(\"log_phi_posterior_loc_g\").detach().exp().cpu()\n",
    "        fig, ax = matplotlib.pyplot.subplots()\n",
    "        ax.hist(phi_g.numpy(),bins=100)\n",
    "        ax.set_xlabel('$\\phi_g$')\n",
    "        ax.set_ylabel('Counts')\n",
    "        \n",
    "        return ax\n",
    "    \n",
    "    def plot_beta_g_distribution(self):\n",
    "        \"\"\"Plot distribution of beta_g\"\"\"\n",
    "        beta_g = pyro.param(\"log_beta_posterior_loc_g\").detach().exp().cpu()\n",
    "        fig, ax = matplotlib.pyplot.subplots()\n",
    "        ax.hist(beta_g.numpy(),bins=100)\n",
    "        ax.set_xlabel('$beta_g$')\n",
    "        ax.set_ylabel('Counts')\n",
    "        \n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-strap",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_anndata_path = \"/home/nbarkas/disk2/deconvolution_method/datasets/ebov/load_data_python/ebov_bulk.h5ad\"\n",
    "sc_anndata_path = \"/home/nbarkas/disk2/deconvolution_method/datasets/ebov/load_data_python/ebov_sc.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bulk_anndata_path, 'rb') as fh:\n",
    "    bulk_anndata  = anndata.read_h5ad(fh)\n",
    "with open(sc_anndata_path, 'rb') as fh:\n",
    "    sc_anndata = anndata.read_h5ad(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select samples only after or on tp 0\n",
    "bulk_anndata = bulk_anndata[bulk_anndata.obs['dpi_time'] >= 0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebov_dataset = DeconvolutionDataset(\n",
    "    sc_anndata = sc_anndata,\n",
    "    sc_celltype_col = \"Subclustering_reduced\",\n",
    "    bulk_anndata = bulk_anndata,\n",
    "    bulk_time_col = \"dpi_time\",\n",
    "    dtype_np = dtype_np,\n",
    "    feature_selection_method = 'overdispersed_bulk_and_high_sc' #'overdispersed_bulk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv = TimeRegularizedDeconvolution(\n",
    "    dataset=ebov_dataset,\n",
    "    polynomial_degree = 20,\n",
    "    #basis_functions = \"legendre\",\n",
    "    basis_functions = \"polynomial\",\n",
    "    device=device,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.fit_model(n_iters=5_000, verbose=True, log_frequency=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-representative",
   "metadata": {},
   "source": [
    "## Examine Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.calculate_composition_trajectories(n_intervals = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.plot_composition_trajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.plot_phi_g_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.plot_beta_g_distribution()\n",
    "matplotlib.pyplot.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-billion",
   "metadata": {},
   "source": [
    "# Synthetic dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  \n",
    "    z = np.exp(-x)\n",
    "    sig = 1 / (1 + z)\n",
    "\n",
    "    return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = generate_batch(pseudo_time_reg_deconv.dataset, device, dtype)\n",
    "batch_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_with_sigmoid_proportions(start_time = -5, end_time = 5, step = 1, num_samples = 100,\n",
    "                                     lib_size_mean = 1e6, lib_size_std = 2e5, use_betas = False):\n",
    "    \"\"\"Simulate bulk data with compositional changes\"\"\"\n",
    "    \n",
    "    # Discrete timepoints to sample from\n",
    "    xs = torch.arange(start_time, end_time, step)\n",
    "    \n",
    "    # Number of celltypes are same as in main deconvolution\n",
    "    num_cell_types = pseudo_time_reg_deconv.w_hat_gc.shape[1]\n",
    "    \n",
    "    # Sample the times for the samples\n",
    "    t_m = xs[torch.randint(len(xs), (num_samples,))]\n",
    "    \n",
    "    # generate the celltype proportions\n",
    "    effect_size = torch.rand(num_cell_types) # 0,1\n",
    "    shift = torch.rand(num_cell_types)\n",
    "    magnitude = torch.where(torch.rand(num_cell_types) < 0.5, -1., 1.)\n",
    "    \n",
    "    # Generate cell population mc\n",
    "    cell_pop_cm = torch.zeros(num_cell_types, num_samples)\n",
    "    for i in range(num_cell_types):\n",
    "        cell_pop_cm[i,:] = torch.Tensor(list(sigmoid(magnitude[i]*x+shift[i]) for x in t_m)) * effect_size[i]\n",
    "    cell_pop_cm = torch.nn.functional.softmax(cell_pop_cm, dim=0)\n",
    "    \n",
    "    # Get phis and betas from main model\n",
    "    # phi_g ~ 0.1 - 0.2\n",
    "    \n",
    "    phi_g = pyro.param(\"log_phi_posterior_loc_g\").detach().exp().cpu()\n",
    "    beta_g = pyro.param(\"log_beta_posterior_loc_g\").detach().exp().cpu()\n",
    "    \n",
    "    # Get celltype profiles from the model\n",
    "    w_hat_gc = pseudo_time_reg_deconv.w_hat_gc.detach().cpu()\n",
    "    if use_betas:\n",
    "        unnorm_w_hat_gc = w_hat_gc * beta_g[:,None]\n",
    "    else:\n",
    "        unnorm_w_hat_gc = w_hat_gc\n",
    "    \n",
    "    # Normalize\n",
    "    w_gc = unnorm_w_hat_gc / unnorm_w_hat_gc.sum(0)\n",
    "    \n",
    "    # Get some library sizes \n",
    "    lib_sizes_m = torch.normal(\n",
    "        mean=torch.full([num_samples], lib_size_mean), \n",
    "        std=torch.full([num_samples], lib_size_std)\n",
    "    )\n",
    "    \n",
    "    # Get the NegBinomial means\n",
    "    # consider: random component on w_gc?\n",
    "    # b_gc -> gene + celltype specific distortion ( how does inference degrade as this increases )\n",
    "    # sample b_gc from laplace (mu = 1, beta(scale) = )\n",
    "    # Gamma(mean = 1, var = 1/rate)\n",
    "    # rate = concentration = a\n",
    "    # 1/a = var of gamma distribution\n",
    "    # sample beta_cg \n",
    "    mu_mg = lib_sizes_m[:,None] * torch.matmul(cell_pop_cm.T, w_gc.transpose(-1, -2))\n",
    "    \n",
    "    # Sample a full matrix using phis from main model\n",
    "    x_ng = NegativeBinomialAltParam(mu=mu_mg, phi = phi_g).sample()\n",
    "    \n",
    "    return {\n",
    "        'cell_pop_cm': cell_pop_cm,\n",
    "        't_m': t_m,\n",
    "        'x_ng': x_ng,\n",
    "        'trajectory_params': {\n",
    "            'type': 'sigmoid',\n",
    "            'effect_size': effect_size,\n",
    "            'shift': shift,\n",
    "            'magnitude': magnitude,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simulated_proportions(sim_res):\n",
    "    \"\"\"Plot simulated proportion results\"\"\"\n",
    "    \n",
    "    \n",
    "    fig, ax = matplotlib.pyplot.subplots()\n",
    "    o = torch.argsort(sim_res['t_m'])\n",
    "    ax.plot(sim_res['t_m'][o], sim_res['cell_pop_cm'][:,o].T)\n",
    "    ax.set_title('Simulated proportions')\n",
    "    ax.set_xlabel('Set time')\n",
    "    ax.set_ylabel('Proportions')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anndata_from_sim(sim_res):\n",
    "    \"\"\"Generate AnnData object from the simulation results\n",
    "    \n",
    "    Time is stored in the time dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    var_tmp = pd.DataFrame({'gene': pseudo_time_reg_deconv.dataset.selected_genes })\n",
    "    var_tmp = var_tmp.set_index('gene')\n",
    "    \n",
    "    return anndata.AnnData(\n",
    "        X = sim_res['x_ng'].numpy(),\n",
    "        var = var_tmp,\n",
    "        obs = pd.DataFrame({'time': sim_res['t_m']})\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-lawsuit",
   "metadata": {},
   "source": [
    "## Evaluate Simulation 1 (100 samples; polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and plot proportions\n",
    "sim_res = simulate_with_sigmoid_proportions(num_samples=100)\n",
    "plot_simulated_proportions(sim_res)\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_bulk = generate_anndata_from_sim(sim_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebov_simulated_dataset = DeconvolutionDataset(\n",
    "    sc_anndata = sc_anndata,\n",
    "    sc_celltype_col = \"Subclustering_reduced\",\n",
    "    bulk_anndata = simulated_bulk,\n",
    "    bulk_time_col = \"time\",\n",
    "    dtype_np = dtype_np,\n",
    "    feature_selection_method = 'common' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim = TimeRegularizedDeconvolution(\n",
    "    dataset=ebov_simulated_dataset,\n",
    "    polynomial_degree = 20,\n",
    "    basis_functions = \"polynomial\",\n",
    "    device=device,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.fit_model(n_iters=20_001, verbose=True, log_frequency=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.calculate_composition_trajectories(n_intervals = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simulated_proportions(sim_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.plot_composition_trajectories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-holmes",
   "metadata": {},
   "source": [
    "## Polynomial low degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim = TimeRegularizedDeconvolution(\n",
    "    dataset=ebov_simulated_dataset,\n",
    "    polynomial_degree = 5,\n",
    "    #basis_functions = \"legendre\",\n",
    "    basis_functions = \"polynomial\",\n",
    "    device=device,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.fit_model(n_iters=20_001, verbose=True, log_frequency=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simulated_proportions(sim_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.calculate_composition_trajectories(n_intervals = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.plot_composition_trajectories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-belfast",
   "metadata": {},
   "source": [
    "### With legendre polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim = TimeRegularizedDeconvolution(\n",
    "    dataset=ebov_simulated_dataset,\n",
    "    polynomial_degree = 3,\n",
    "    basis_functions = \"legendre\",\n",
    "    device=device,\n",
    "    dtype=dtype)\n",
    "pseudo_time_reg_deconv_sim.fit_model(n_iters=20_001, verbose=True, log_frequency=1000)\n",
    "pseudo_time_reg_deconv_sim.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.calculate_composition_trajectories(n_intervals = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simulated_proportions(sim_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.plot_composition_trajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Many ledenre polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim = TimeRegularizedDeconvolution(\n",
    "    dataset=ebov_simulated_dataset,\n",
    "    polynomial_degree = 15,\n",
    "    basis_functions = \"legendre\",\n",
    "    device=device,\n",
    "    dtype=dtype)\n",
    "pseudo_time_reg_deconv_sim.fit_model(n_iters=20_001, verbose=True, log_frequency=1000)\n",
    "pseudo_time_reg_deconv_sim.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.calculate_composition_trajectories(n_intervals = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simulated_proportions(sim_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.plot_composition_trajectories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-third",
   "metadata": {},
   "source": [
    "# Without betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim = TimeRegularizedDeconvolution(\n",
    "    dataset=ebov_simulated_dataset,\n",
    "    polynomial_degree = 20,\n",
    "    #basis_functions = \"legendre\",\n",
    "    basis_functions = \"polynomial\",\n",
    "    use_betas = False,\n",
    "    device=device,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.fit_model(n_iters=20_001, verbose=True, log_frequency=1000)\n",
    "pseudo_time_reg_deconv_sim.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simulated_proportions(sim_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.calculate_composition_trajectories(n_intervals = 1000)\n",
    "pseudo_time_reg_deconv_sim.plot_composition_trajectories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-netherlands",
   "metadata": {},
   "source": [
    "## Error Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_error_L1(sim_res, pseudo_time_reg_deconv_sim, n_intervals = 10):\n",
    "\n",
    "    ## Get the ground truth\n",
    "    if sim_res['trajectory_params']['type'] == 'sigmoid':\n",
    "        start_time = -5\n",
    "        end_time = 5\n",
    "        step = (end_time - start_time) / n_intervals\n",
    "        #step = 1\n",
    "        t_m = torch.arange(start_time, end_time, step)\n",
    "        magnitude = sim_res['trajectory_params']['magnitude']\n",
    "        shift = sim_res['trajectory_params']['shift']\n",
    "        effect_size = sim_res['trajectory_params']['effect_size']\n",
    "        num_cell_types = sim_res['trajectory_params']['effect_size'].shape[0]\n",
    "        num_samples = t_m.shape[0]\n",
    "        cell_pop_cm = torch.zeros(num_cell_types, num_samples)\n",
    "        for i in range(num_cell_types):\n",
    "            cell_pop_cm[i,:] = torch.Tensor(list(sigmoid(magnitude[i]*x+shift[i]) for x in t_m)) * effect_size[i]\n",
    "        ground_truth_proportions_cm = torch.nn.functional.softmax(cell_pop_cm, dim=0).T\n",
    "    else:\n",
    "        raise Error('Unknown trajectory type')\n",
    "        \n",
    "    # Get the predictions\n",
    "    pseudo_time_reg_deconv_sim.calculate_composition_trajectories(n_intervals = n_intervals)\n",
    "    ret_vals = pseudo_time_reg_deconv_sim.calculated_trajectories\n",
    "    \n",
    "    L1_error = (ground_truth_proportions_cm - ret_vals['norm_comp_t']).abs().sum([0,1])\n",
    "    L1_error_norm = L1_error / n_intervals\n",
    "\n",
    "    return L1_error_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_error_L1(sim_res, pseudo_time_reg_deconv_sim, n_intervals = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-trust",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-shell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample on the t_m (LOO or range)\n",
    "# training and generalization accuracy \n",
    "# generalizatoin accuracy vs time\n",
    "# discontiuity and sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and generalization accuracy for different gene and cell distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-mounting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-chance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-participant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-albert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-stress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-winner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-touch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-pocket",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
