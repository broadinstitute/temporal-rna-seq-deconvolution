{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "funky-probability",
   "metadata": {},
   "source": [
    "## Ebola infected Macaque Sample Composition Trajectory Identification\n",
    "\n",
    "```\n",
    "Indices:\n",
    "\n",
    "- c cell type\n",
    "- g genes\n",
    "- m samples\n",
    "- k deformation polynomial degree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pylab as plt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "from typing import Dict\n",
    "from pyro.distributions.torch_distribution import TorchDistribution, TorchDistributionMixin\n",
    "from torch.distributions.utils import probs_to_logits, logits_to_probs, broadcast_all, lazy_property\n",
    "from torch.distributions import constraints\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from typing import List, Dict\n",
    "from boltons.cacheutils import cachedproperty\n",
    "from pyro.distributions.torch_distribution import TorchDistribution, TorchDistributionMixin\n",
    "from torch.distributions.utils import probs_to_logits, logits_to_probs, broadcast_all, lazy_property\n",
    "from torch.distributions import constraints\n",
    "from numbers import Number\n",
    "import pyro.distributions as dist\n",
    "import anndata\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, SplineTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-madison",
   "metadata": {},
   "source": [
    "## Parameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float32\n",
    "dtype_np = np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-nirvana",
   "metadata": {},
   "source": [
    "## Helper functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import legendre\n",
    "\n",
    "def legendre_coefficient_mat(k_max, epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Return the coefficient matrix of legendre polynomials.\n",
    "    \n",
    "    :param k_max: legenre polynomial max degree\n",
    "    :param epsilon: minimum coefficient value\n",
    "    \"\"\"\n",
    "    \n",
    "    k_max_internal = k_max + 1\n",
    "    X_kl = torch.zeros(k_max_internal, k_max_internal)\n",
    "    for i in range(0, k_max_internal):\n",
    "        terms = list(legendre(i))\n",
    "        terms.reverse()\n",
    "        X_kl[i,:i+1] = torch.tensor(terms,dtype=dtype)\n",
    "    X_kl = torch.where(X_kl.abs() < epsilon, torch.zeros_like(X_kl), X_kl)\n",
    "    return X_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegativeBinomialAltParam(mu, phi):\n",
    "    \"\"\"\n",
    "    Creates a negative binomial distribution.\n",
    "    \n",
    "    Args:\n",
    "        mu (Number, Tensor): mean (must be strictly positive)\n",
    "        phi (Number, Tensor): overdispersion (must be strictly positive)\n",
    "    \"\"\"\n",
    "    return dist.GammaPoisson(\n",
    "            concentration= 1 /phi ,\n",
    "            rate=1 / (mu * phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvolutionDataset:\n",
    "    \"\"\"This class represents a bulk and single-cell dataset to be deconvolved in tandem\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            sc_anndata: anndata.AnnData,\n",
    "            sc_celltype_col: str,\n",
    "            bulk_anndata: anndata.AnnData,\n",
    "            bulk_time_col: str,\n",
    "            dtype_np: np.dtype,\n",
    "            feature_selection_method: str = \"common\"):\n",
    "\n",
    "        self.dtype_np = dtype_np\n",
    "        self.sc_celltype_col = sc_celltype_col\n",
    "        self.bul_time_col = bulk_time_col\n",
    "\n",
    "        self.selected_genes = ()\n",
    "        #self.bulk_anndata = bulk_anndata\n",
    "        #self.sc_anndata = sc_anndata\n",
    "        \n",
    "        # Select common genes and subset/order anndata objects\n",
    "        selected_genes = self.__select_features(bulk_anndata, sc_anndata, feature_selection_method = feature_selection_method)\n",
    "        \n",
    "        \n",
    "        self.bulk_anndata = bulk_anndata[:,selected_genes]\n",
    "        self.sc_anndata = sc_anndata[:,selected_genes]\n",
    "        \n",
    "        self.num_genes = len(selected_genes)\n",
    "        \n",
    "        # TODO: Issue warning if too many genes removed\n",
    "        \n",
    "        # Pre-process time values and save inverse function\n",
    "        self.dpi_time_original_m = self.bulk_anndata.obs[bulk_time_col].values.astype(dtype_np)\n",
    "        self.time_min = np.min(self.dpi_time_original_m)\n",
    "        self.time_range = np.max(self.dpi_time_original_m) - self.time_min\n",
    "        self.dpi_time_m = (self.dpi_time_original_m - self.time_min) / self.time_range\n",
    "        \n",
    "        \n",
    "    def __select_features(self, bulk_anndata, sc_anndata, feature_selection_method, dispersion_cutoff = 5):\n",
    "        print(f'feature selection method: {feature_selection_method}')\n",
    "        \n",
    "        if feature_selection_method == \"common\":\n",
    "            self.selected_genes = list(set(bulk_anndata.var.index).intersection(set(sc_anndata.var.index)))\n",
    "        elif feature_selection_method == 'overdispersed_bulk':\n",
    "            x_train = np.log(bulk_anndata.X.mean(0)+1) # log_mu_g\n",
    "            y_train = np.log(bulk_anndata.X.var(0)+1) # log_sigma_g\n",
    "            \n",
    "            X_train = x_train[:, np.newaxis]\n",
    "            degree = 3\n",
    "            model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=1e-3))\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_train)\n",
    "            \n",
    "            # Select Genes\n",
    "            sel_over = (y_train - y_pred > 0.0) & (y_train > dispersion_cutoff)\n",
    "            self.selected_genes = list(bulk_anndata.var.index[sel_over])\n",
    "            \n",
    "        elif feature_selection_method == 'overdispersed_bulk_and_high_sc':\n",
    "            # Fit polynomial degree\n",
    "            polynomial_degree = 2\n",
    "            sc_cutoff = 2 # log scale\n",
    "            \n",
    "            # Select overdispersed in bulk\n",
    "            x_train = np.log(bulk_anndata.X.mean(0)+1) # log_mu_g\n",
    "            y_train = np.log(bulk_anndata.X.var(0)+1) # log_sigma_g\n",
    "            \n",
    "            X_train = x_train[:, np.newaxis]\n",
    "            \n",
    "            model = make_pipeline(PolynomialFeatures(polynomial_degree), Ridge(alpha=1e-3))\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_train)\n",
    "            \n",
    "            # Select Genes\n",
    "            sel_over_bulk = (y_train - y_pred > 0.0) & (y_train > dispersion_cutoff)\n",
    "            selected_genes_bulk = set(bulk_anndata.var.index[sel_over_bulk])\n",
    "            \n",
    "            # Select highly-expressed in single-cell\n",
    "            \n",
    "            selected_genes_sc = set(sc_anndata.var.index[np.log(sc_anndata.X.sum(0)+1) > sc_cutoff])\n",
    "            \n",
    "            self.selected_genes = list(selected_genes_bulk.intersection(selected_genes_sc))\n",
    "            \n",
    "\n",
    "        print(f'selected {len(self.selected_genes)} genes')\n",
    "            \n",
    "        return self.selected_genes\n",
    "\n",
    "# Some plotting code\n",
    "#             sel_under = (y_train - y_pred < 0.0 ) | (y_train <= 7)\n",
    "#             lw = 1e-4\n",
    "#             fig, ax = matplotlib.pyplot.subplots()\n",
    "#             ax.scatter(x_train[sel_under], y_train[sel_under], linewidth=lw, label=\"ground truth\", c='black', marker='.', s=2)\n",
    "#             ax.scatter(x_train[sel_over], y_train[sel_over], linewidth=lw, label=\"ground truth\", c='orange', marker='.', s=2)\n",
    "#             # order for the line\n",
    "#             o = np.argsort(X_train.T)[0]\n",
    "#             matplotlib.pyplot.plot(X_train[o,:], y_pred[o], c='red')\n",
    "\n",
    "        \n",
    "    @cachedproperty\n",
    "    def cell_type_str_list(self) -> List[str]:\n",
    "        return sorted(list(set(self.sc_anndata.obs[self.sc_celltype_col])))\n",
    "    \n",
    "    @cachedproperty\n",
    "    def cell_type_str_to_index_map(self) -> Dict[str, int]:\n",
    "        return {cell_type_str: index for index, cell_type_str in enumerate(self.cell_type_str_list)}        \n",
    "    \n",
    "    @cachedproperty\n",
    "    def num_cell_types(self) -> int:\n",
    "        return len(self.cell_type_str_list)\n",
    "    \n",
    "    @cachedproperty\n",
    "    def w_hat_gc(self) -> np.ndarray:\n",
    "        \"\"\"Calculate the estimate cell profiles\"\"\"\n",
    "        w_hat_gc = np.zeros((self.num_genes, self.num_cell_types))\n",
    "        for cell_type_str in self.cell_type_str_list:\n",
    "            i_cell_type = self.cell_type_str_to_index_map[cell_type_str]\n",
    "            mask_j = self.sc_anndata.obs[self.sc_celltype_col].values == cell_type_str\n",
    "            w_hat_gc[:, i_cell_type] = np.sum(self.sc_anndata.X[mask_j,:], axis=-2)\n",
    "            w_hat_gc[:, i_cell_type] = w_hat_gc[:, i_cell_type] / np.sum(w_hat_gc[:, i_cell_type])\n",
    "        return w_hat_gc\n",
    "    \n",
    "    @cachedproperty\n",
    "    def bulk_raw_gex_mg(self) -> torch.tensor:\n",
    "        return torch.tensor(self.bulk_anndata.X, device=device, dtype=dtype)\n",
    "    \n",
    "    @cachedproperty\n",
    "    def t_m(self) -> torch.tensor:\n",
    "        return torch.tensor(self.dpi_time_m, device=device, dtype=dtype)\n",
    "    \n",
    "#     def num_bulk_samples() -> int:\n",
    "#         return self.bulk_anndata.X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    dataset: DeconvolutionDataset,\n",
    "    device: torch.device,\n",
    "    dtype: torch.dtype):\n",
    "    \n",
    "    return {\n",
    "        \"x_mg\": torch.tensor(dataset.bulk_raw_gex_mg, device=device, dtype=dtype),\n",
    "        \"t_m\": torch.tensor(dataset.dpi_time_m, device=device, dtype=dtype),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-economy",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRegularizedDeconvolution:\n",
    "    \n",
    "    def  __init__(\n",
    "            self,\n",
    "            dataset: DeconvolutionDataset,\n",
    "            basis_functions: str,\n",
    "            polynomial_degree: int,\n",
    "            device: device,\n",
    "            dtype: dtype):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.polynomial_degree = polynomial_degree\n",
    "        self.basis_functions = basis_functions\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.log_beta_prior_scale = 1.0\n",
    "        self.log_r_prior_scale = 1.0\n",
    "        self.tau_prior_scale = 1.0\n",
    "        self.log_phi_prior_loc = -5.0\n",
    "        self.log_phi_prior_scale = 1.0\n",
    "        \n",
    "        self.unnorm_cell_pop_base_prior_loc_c = np.zeros((self.dataset.num_cell_types,))\n",
    "        self.unnorm_cell_pop_base_prior_scale_c = np.ones((self.dataset.num_cell_types,))\n",
    "\n",
    "        # dist of coefficients of population deformation polynomial\n",
    "        self.unnorm_cell_pop_deform_prior_loc_ck = np.zeros((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "        self.unnorm_cell_pop_deform_prior_scale_ck = np.ones((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "\n",
    "\n",
    "        self.init_posterior_global_scale_factor = 0.05\n",
    "        \n",
    "        self.log_beta_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.log_r_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.tau_posterior_scale = 1.0 * self.init_posterior_global_scale_factor\n",
    "        self.log_phi_posterior_loc = -5.0\n",
    "        self.log_phi_posterior_scale = 0.1 * self.init_posterior_global_scale_factor\n",
    "        \n",
    "        \n",
    "        self.unnorm_cell_pop_base_posterior_loc_c = np.zeros((self.dataset.num_cell_types,))\n",
    "        self.unnorm_cell_pop_base_posterior_scale_c = self.init_posterior_global_scale_factor * np.ones((self.dataset.num_cell_types,))\n",
    "        \n",
    "        self.unnorm_cell_pop_deform_posterior_loc_ck = np.zeros((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "        self.unnorm_cell_pop_deform_posterior_scale_ck = self.init_posterior_global_scale_factor *  np.ones((self.dataset.num_cell_types, self.polynomial_degree))\n",
    "        \n",
    "        # cache useful tensors\n",
    "        self.w_hat_gc = torch.tensor(self.dataset.w_hat_gc, device=device, dtype=dtype)\n",
    "        \n",
    "    def model(\n",
    "            self,\n",
    "            x_mg: torch.Tensor,\n",
    "            t_m: torch.Tensor):\n",
    "        \"\"\"Main model\n",
    "        \n",
    "        :param x_mg: gene expression\n",
    "        :param t_m: obseration time\n",
    "        \"\"\"\n",
    "        \n",
    "        # sample log_phi_g\n",
    "        log_phi_g = pyro.sample(\n",
    "            'log_phi_g',\n",
    "            dist.Normal(\n",
    "                loc=self.log_phi_prior_loc * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype),\n",
    "                scale=self.log_phi_prior_scale * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype)).to_event(1))\n",
    "        \n",
    "        # sample log_beta_g\n",
    "        log_beta_g = pyro.sample(\n",
    "            'log_beta_g',\n",
    "            dist.Normal(\n",
    "                loc=torch.zeros(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype),\n",
    "                scale=self.log_beta_prior_scale * torch.ones(\n",
    "                    (self.dataset.num_genes,), device=self.device, dtype=self.dtype)).to_event(1))\n",
    "        \n",
    "        # sample unnorm_cell_pop_base_c\n",
    "        unnorm_cell_pop_base_c = pyro.sample(\n",
    "            'unnorm_cell_pop_base_c',\n",
    "            dist.Normal(\n",
    "                loc=torch.tensor(\n",
    "                    self.unnorm_cell_pop_base_prior_loc_c,\n",
    "                    device=self.device, dtype=self.dtype),\n",
    "                scale=torch.tensor(\n",
    "                    self.unnorm_cell_pop_base_prior_scale_c,\n",
    "                    device=self.device, dtype=self.dtype)).to_event(1))\n",
    "        \n",
    "        # Deformation scale is a learnable parameter now\n",
    "        unnorm_cell_pop_deform_prior_scale_ck = pyro.param(\"unnorm_cell_pop_deform_prior_scale_ck\",\n",
    "                        torch.tensor(self.unnorm_cell_pop_deform_prior_scale_ck, \n",
    "                        device=self.device,\n",
    "                        dtype=self.dtype),                          \n",
    "                        constraint = constraints.positive) \n",
    "        \n",
    "        unnorm_cell_pop_deform_ck = pyro.sample(\n",
    "            'unnorm_cell_pop_deform_ck',\n",
    "            dist.Normal(\n",
    "                loc=torch.tensor(\n",
    "                    self.unnorm_cell_pop_deform_prior_loc_ck,\n",
    "                    device=self.device, dtype=self.dtype),\n",
    "                scale=unnorm_cell_pop_deform_prior_scale_ck).to_event(2))\n",
    "        \n",
    "        # calculate useful derived variables\n",
    "        beta_g = log_beta_g.exp()\n",
    "        phi_g = log_phi_g.exp()\n",
    "        \n",
    "        # Get normalized w_gc\n",
    "        #unnorm_w_gc = self.w_hat_gc * beta_g[:, None]\n",
    "        unnorm_w_gc = self.w_hat_gc\n",
    "        w_gc = unnorm_w_gc / unnorm_w_gc.sum(0)\n",
    "\n",
    "        if self.basis_functions == 'polynomial':\n",
    "            tau_km = torch.pow(t_m[None,:], torch.arange(1,self.polynomial_degree + 1, device=self.device)[:,None])  \n",
    "            deformation_mc = torch.matmul(unnorm_cell_pop_deform_ck, tau_km).transpose(-1,-2)\n",
    "        elif self.basis_functions == 'legendre':\n",
    "            # l -- power of the term of the legrenre polynomial\n",
    "            t_m_prime = 2 * t_m - 1  # discrete times in (-1,1)\n",
    "            t_lm = torch.pow(t_m_prime[None,:], torch.arange(0,self.polynomial_degree + 1, device=self.device)[:,None])\n",
    "            c_kl = legendre_coefficient_mat(self.polynomial_degree)[1:,].to(device) # drop constant term\n",
    "            intermediate_legenre_vals_km = torch.matmul(c_kl, t_lm)\n",
    "            deformation_mc = torch.matmul(unnorm_cell_pop_deform_ck, intermediate_legenre_vals_km).transpose(-1,-2)\n",
    "            \n",
    "        # TODO: add a sample specific proportion sampling from the corresponding time point\n",
    "        \n",
    "        cell_pop_unnorm_mc = unnorm_cell_pop_base_c[None,:] + deformation_mc\n",
    "            \n",
    "        # Normalize the cell proportions\n",
    "        cell_pop_mc = torch.nn.functional.softmax(cell_pop_unnorm_mc, dim=-1)\n",
    "        \n",
    "        # calculate mean gene expression\n",
    "        mu_mg = x_mg.sum(-1)[:, None] * torch.matmul(cell_pop_mc, w_gc.transpose(-1, -2))\n",
    "\n",
    "        with pyro.plate('batch'):\n",
    "            # observe gene expression\n",
    "            pyro.sample(\n",
    "                \"x_mg\",\n",
    "                NegativeBinomialAltParam(\n",
    "                    mu=mu_mg,\n",
    "                    phi=phi_g[None, :]).to_event(1),\n",
    "                obs=x_mg)\n",
    "\n",
    "    def delta_guide(\n",
    "            self,\n",
    "            x_mg: torch.Tensor,\n",
    "            t_m: torch.Tensor):\n",
    "        \"\"\"Simple delta guide\"\"\"\n",
    "        \n",
    "        # variational parameters for log_phi_g\n",
    "        log_phi_posterior_loc_g = pyro.param(\n",
    "            \"log_phi_posterior_loc_g\",\n",
    "            self.log_phi_posterior_loc * torch.ones(\n",
    "                (self.dataset.num_genes,), device=self.device, dtype=self.dtype))\n",
    "\n",
    "        # variational parameters for log_beta_g\n",
    "        log_beta_posterior_loc_g = pyro.param(\n",
    "            \"log_beta_posterior_loc_g\",\n",
    "            torch.zeros(\n",
    "                (self.dataset.num_genes,), device=self.device, dtype=self.dtype))        \n",
    "        \n",
    "        # variational parameters for unnorm_cell_pop_base_c (\"B_c\")\n",
    "        unnorm_cell_pop_base_posterior_loc_c = pyro.param(\n",
    "            \"unnorm_cell_pop_base_posterior_loc_c\",\n",
    "            torch.tensor(\n",
    "                self.unnorm_cell_pop_base_posterior_loc_c,\n",
    "                device=self.device, dtype=self.dtype))\n",
    "        \n",
    "        # variational parameters for unnorm_cell_pop_deform_c (\"R_c\")\n",
    "        unnorm_cell_pop_deform_posterior_loc_ck = pyro.param(\n",
    "            \"unnorm_cell_pop_deform_posterior_loc_ck\",\n",
    "            torch.tensor(\n",
    "                self.unnorm_cell_pop_deform_posterior_loc_ck,\n",
    "                device=self.device, dtype=self.dtype))\n",
    "        \n",
    "        # posterior sample statements\n",
    "        log_phi_g = pyro.sample(\n",
    "            \"log_phi_g\",\n",
    "            dist.Delta(\n",
    "                v=log_phi_posterior_loc_g).to_event(1))\n",
    "\n",
    "        log_beta_g = pyro.sample(\n",
    "            \"log_beta_g\",\n",
    "            dist.Delta(\n",
    "                v=log_beta_posterior_loc_g).to_event(1))\n",
    "\n",
    "        unnorm_cell_pop_base_c = pyro.sample(\n",
    "            'unnorm_cell_pop_base_c',\n",
    "            dist.Delta(\n",
    "                v=unnorm_cell_pop_base_posterior_loc_c).to_event(1))\n",
    "        \n",
    "        unnorm_cell_pop_deform_ck = pyro.sample(\n",
    "            'unnorm_cell_pop_deform_ck',\n",
    "            dist.Delta(v=unnorm_cell_pop_deform_posterior_loc_ck).to_event(2))\n",
    "        \n",
    "        \n",
    "    def fit_model(self, n_iters = 3000, log_frequency = 100, verbose = True, clear_param_store = True):\n",
    "        if clear_param_store:\n",
    "            pyro.clear_param_store()\n",
    "            \n",
    "        ## TODO: bring these out\n",
    "        optim = pyro.optim.Adam({'lr': 1e-3})\n",
    "        \n",
    "        self.loss_hist = []\n",
    "        \n",
    "        svi = SVI(\n",
    "            model=self.model,\n",
    "            guide=self.delta_guide,\n",
    "            optim=optim,\n",
    "            loss=Trace_ELBO())\n",
    "        \n",
    "        for i_iter in range(n_iters):\n",
    "            batch_dict = generate_batch(self.dataset, device, dtype)\n",
    "            loss = svi.step(**batch_dict)\n",
    "            self.loss_hist.append(loss)\n",
    "            if verbose:\n",
    "                if i_iter % log_frequency == 0:\n",
    "                    print(f\"[iteration: {i_iter}]   loss: {self.loss_hist[-1]:.2f}\")\n",
    "                \n",
    "    def plot_loss(self):\n",
    "        \"\"\"Plot the losses during training\"\"\"\n",
    "        fig, ax = matplotlib.pyplot.subplots()\n",
    "        ax.plot(self.loss_hist)\n",
    "        ax.set_title('Losses')\n",
    "        ax.set_xlabel('iteration')\n",
    "        ax.set_ylabel('ELBO Loss')\n",
    "        \n",
    "        return ax\n",
    "    \n",
    "    \n",
    "    def calculate_composition_trajectories(self, time_step=0.01):\n",
    "        \"\"\"Calculate the composition trajectories\"\"\"\n",
    "\n",
    "        \n",
    "        # calculate true times\n",
    "        if self.basis_functions == 'polynomial':\n",
    "            times_z = torch.arange(0,1,time_step)\n",
    "        \n",
    "            # Take time to appropriate exponent\n",
    "            times_zk = torch.pow(times_z[:,None], torch.arange(1,self.polynomial_degree + 1,))\n",
    "\n",
    "            # get the trained params\n",
    "            base_composition_post_c = pyro.param(\"unnorm_cell_pop_base_posterior_loc_c\").detach().cpu()\n",
    "            delta_composition_post_ck = pyro.param(\"unnorm_cell_pop_deform_posterior_loc_ck\").detach().cpu()\n",
    "\n",
    "            # Calculate the deltas for each time point\n",
    "            delta_cz = torch.matmul(delta_composition_post_ck, times_zk.transpose(-1,-2))\n",
    "\n",
    "            # normalize\n",
    "            norm_comp_t = torch.nn.functional.softmax(base_composition_post_c[:,None] + delta_cz, dim=0).numpy().T\n",
    "            \n",
    "            true_times_z = times_z * self.dataset.time_range + self.dataset.time_min\n",
    "        elif self.basis_functions == 'legendre':\n",
    "            times_z = torch.arange(-1,1,time_step)\n",
    "\n",
    "            # Take time to appropriate exponent\n",
    "            times_zk = torch.pow(times_z[:,None], torch.arange(1,self.polynomial_degree + 1,))\n",
    "\n",
    "            # get the trained params\n",
    "            base_composition_post_c = pyro.param(\"unnorm_cell_pop_base_posterior_loc_c\").detach().cpu()\n",
    "            delta_composition_post_ck = pyro.param(\"unnorm_cell_pop_deform_posterior_loc_ck\").detach().cpu()\n",
    "\n",
    "            # Calculate the deltas for each time point\n",
    "            delta_cz = torch.matmul(delta_composition_post_ck, times_zk.transpose(-1,-2))\n",
    "\n",
    "            # normalize\n",
    "            norm_comp_t = torch.nn.functional.softmax(base_composition_post_c[:,None] + delta_cz, dim=0).numpy().T\n",
    "            \n",
    "            true_times_z = ((times_z+1)/2) * self.dataset.time_range + self.dataset.time_min\n",
    "        \n",
    "        self.calculated_trajectories = {\n",
    "            \"times_z\": times_z.numpy(), \n",
    "            \"true_times_z\": true_times_z,\n",
    "            \"norm_comp_t\": norm_comp_t\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def get_composition_trajectories(self):\n",
    "        \"\"\"Return the composition trajectories\"\"\"\n",
    "        return self.calculated_trajectories\n",
    "    \n",
    "    \n",
    "    def plot_composition_trajectories(self):\n",
    "        \"\"\"Plot the composition trajectories\"\"\"\n",
    "        fig, ax  = matplotlib.pyplot.subplots()\n",
    "        ax.plot(self.calculated_trajectories[\"true_times_z\"], self.calculated_trajectories[\"norm_comp_t\"])\n",
    "        ax.set_title(\"Predicted cell proportions\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.legend(ebov_dataset.cell_type_str_list,loc='best',fontsize='small')\n",
    "        \n",
    "    def plot_phi_g_distribution(self):\n",
    "        \"\"\"Plot the distribution of phi_g\"\"\"\n",
    "        phi_g = pyro.param(\"log_phi_posterior_loc_g\").detach().exp().cpu()\n",
    "        fig, ax = matplotlib.pyplot.subplots()\n",
    "        ax.hist(phi_g.numpy(),bins=100)\n",
    "        ax.set_xlabel('$\\phi_g$')\n",
    "        ax.set_ylabel('Counts')\n",
    "        \n",
    "        return ax\n",
    "    \n",
    "    def plot_beta_g_distribution(self):\n",
    "        \"\"\"Plot distribution of beta_g\"\"\"\n",
    "        beta_g = pyro.param(\"log_beta_posterior_loc_g\").detach().exp().cpu()\n",
    "        fig, ax = matplotlib.pyplot.subplots()\n",
    "        ax.hist(beta_g.numpy(),bins=100)\n",
    "        ax.set_xlabel('$beta_g$')\n",
    "        ax.set_ylabel('Counts')\n",
    "        \n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-hacker",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_anndata_path = \"/home/nbarkas/disk2/deconvolution_method/datasets/ebov/load_data_python/ebov_bulk.h5ad\"\n",
    "sc_anndata_path = \"/home/nbarkas/disk2/deconvolution_method/datasets/ebov/load_data_python/ebov_sc.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bulk_anndata_path, 'rb') as fh:\n",
    "    bulk_anndata  = anndata.read_h5ad(fh)\n",
    "with open(sc_anndata_path, 'rb') as fh:\n",
    "    sc_anndata = anndata.read_h5ad(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select samples only after or on tp 0\n",
    "bulk_anndata = bulk_anndata[bulk_anndata.obs['dpi_time'] >= 0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebov_dataset = DeconvolutionDataset(\n",
    "    sc_anndata = sc_anndata,\n",
    "    sc_celltype_col = \"Subclustering_reduced\",\n",
    "    bulk_anndata = bulk_anndata,\n",
    "    bulk_time_col = \"dpi_time\",\n",
    "    dtype_np = dtype_np,\n",
    "    feature_selection_method = 'overdispersed_bulk_and_high_sc' #'overdispersed_bulk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv = TimeRegularizedDeconvolution(\n",
    "    dataset=ebov_dataset,\n",
    "    polynomial_degree = 20,\n",
    "    #basis_functions = \"legendre\",\n",
    "    basis_functions = \"polynomial\",\n",
    "    device=device,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.fit_model(n_iters=5_000, verbose=True, log_frequency=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-jamaica",
   "metadata": {},
   "source": [
    "## Examine Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.calculate_composition_trajectories(time_step=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.plot_composition_trajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.plot_phi_g_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv.plot_beta_g_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-serum",
   "metadata": {},
   "source": [
    "# Synthetic dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  \n",
    "    z = np.exp(-x)\n",
    "    sig = 1 / (1 + z)\n",
    "\n",
    "    return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = generate_batch(pseudo_time_reg_deconv.dataset, device, dtype)\n",
    "batch_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_with_sigmoid_proportions(start_time = -5, end_time = 5, step = 1, num_samples = 100,\n",
    "                                     lib_size_mean = 1e6, lib_size_std = 2e5):\n",
    "    \"\"\"Simulate bulk data with compositional changes\"\"\"\n",
    "    \n",
    "    # Discrete timepoints to sample from\n",
    "    xs = torch.arange(start_time, end_time, step)\n",
    "    \n",
    "    # Number of celltypes are same as in main deconvolution\n",
    "    num_cell_types = pseudo_time_reg_deconv.w_hat_gc.shape[1]\n",
    "    \n",
    "    # Sample the times for the samples\n",
    "    t_m = xs[torch.randint(len(xs), (num_samples,))]\n",
    "    \n",
    "    # generate the celltype proportions\n",
    "    effect_size = torch.rand(num_cell_types)\n",
    "    shift = torch.rand(num_cell_types)\n",
    "    magnitude = torch.where(torch.rand(num_cell_types) < 0.5, -1., 1.)\n",
    "    cell_pop_cm = torch.zeros(num_cell_types, num_samples)\n",
    "    for i in range(num_cell_types):\n",
    "        cell_pop_cm[i,:] = torch.Tensor(list(sigmoid(magnitude[i]*x+shift[i]) for x in t_m)) * effect_size[i]\n",
    "    cell_pop_cm = torch.nn.functional.softmax(cell_pop_cm, dim=0)\n",
    "    \n",
    "    # Get phis and betas from main model\n",
    "    phi_g = pyro.param(\"log_phi_posterior_loc_g\").detach().exp().cpu()\n",
    "    beta_g = pyro.param(\"log_beta_posterior_loc_g\").detach().exp().cpu()\n",
    "    \n",
    "    # Get celltype profiles from the model\n",
    "    w_hat_gc = pseudo_time_reg_deconv.w_hat_gc.detach().cpu()\n",
    "    unnorm_w_hat_gc = w_hat_gc\n",
    "    #unnorm_w_hat_gc = w_hat_gc * beta_g[:,None]\n",
    "    w_gc = unnorm_w_hat_gc / unnorm_w_hat_gc.sum(0)\n",
    "    \n",
    "    # Get some library sizes \n",
    "    lib_sizes_m = torch.normal(\n",
    "        mean=torch.full([num_samples], lib_size_mean), \n",
    "        std=torch.full([num_samples], lib_size_std)\n",
    "    )\n",
    "    \n",
    "    # Get the NegBinomial means\n",
    "    mu_mg = lib_sizes_m[:,None] * torch.matmul(cell_pop_cm.T, w_gc.transpose(-1, -2))\n",
    "    \n",
    "    # Sample a full matrix using phis from main model\n",
    "    x_ng = NegativeBinomialAltParam(mu=mu_mg, phi = phi_g).sample()\n",
    "    \n",
    "    return {\n",
    "        'cell_pop_cm': cell_pop_cm,\n",
    "        't_m': t_m,\n",
    "        'x_ng': x_ng,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_res = simulate_with_sigmoid_proportions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simulated_proportions(sim_res):\n",
    "    fig, ax = matplotlib.pyplot.subplots()\n",
    "    o = torch.argsort(sim_res['t_m'])\n",
    "    ax.plot(sim_res['t_m'][o], sim_res['cell_pop_cm'][:,o].T)\n",
    "    ax.set_title('Simulated proportions')\n",
    "    ax.set_xlabel('Set time')\n",
    "    ax.set_ylabel('Proportions')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simulated_proportions(sim_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anndata_from_sim(sim_res):\n",
    "    \n",
    "    var_tmp = pd.DataFrame({'gene': pseudo_time_reg_deconv.dataset.selected_genes })\n",
    "    var_tmp = var_tmp.set_index('gene')\n",
    "    \n",
    "    return anndata.AnnData(\n",
    "        X = sim_res['x_ng'].numpy(),\n",
    "        var = var_tmp,\n",
    "        obs = pd.DataFrame({'time': sim_res['t_m']})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_bulk = generate_anndata_from_sim(sim_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebov_simulated_dataset = DeconvolutionDataset(\n",
    "    sc_anndata = sc_anndata,\n",
    "    sc_celltype_col = \"Subclustering_reduced\",\n",
    "    bulk_anndata = simulated_bulk,\n",
    "    bulk_time_col = \"time\",\n",
    "    dtype_np = dtype_np,\n",
    "    feature_selection_method = 'common' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim = TimeRegularizedDeconvolution(\n",
    "    dataset=ebov_simulated_dataset,\n",
    "    polynomial_degree = 20,\n",
    "    #basis_functions = \"legendre\",\n",
    "    basis_functions = \"polynomial\",\n",
    "    device=device,\n",
    "    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.fit_model(n_iters=5_000, verbose=True, log_frequency=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.calculate_composition_trajectories(time_step=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_time_reg_deconv_sim.plot_composition_trajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-executive",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
